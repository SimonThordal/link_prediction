{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import scipy.sparse.linalg as splin\n",
    "import random\n",
    "import math\n",
    "import heapq\n",
    "from sklearn.externals.six import StringIO\n",
    "import pydotplus\n",
    "import igraph as ig\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score, make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import RFECV, SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from collections import *\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, SVR\n",
    "from scipy.sparse import dok_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run load_data.py\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "# Find the greatest connected component and work on that\n",
    "components = []\n",
    "lengths = []\n",
    "# Find the greatest component from the undirected version of the graph\n",
    "for component in nx.connected_component_subgraphs(nx.Graph(G)):\n",
    "    components.append(component)\n",
    "    lengths.append(len(component))\n",
    "# Find the GCC as the largest component and then recreate the directed graph\n",
    "GCC = components[lengths.index(max(lengths))]\n",
    "GCC = G.subgraph(GCC.nodes())\n",
    "\n",
    "def successors_scoped(node, head, G):\n",
    "    \"\"\"\n",
    "    Return the successors of a node scoped to the time specified by the head node\n",
    "    \n",
    "    arguments:\n",
    "    node -- node to return successors for\n",
    "    head -- node containing the timestamp to scope the graph to\n",
    "    G -- graph containing the nodes\n",
    "    \"\"\"\n",
    "    max_time = G.node[head]['date']\n",
    "    return [n for n in G[node] if G.node[n]['date'] <= max_time]\n",
    "\n",
    "def predecessors_scoped(node, head, G):\n",
    "    \"\"\"\n",
    "    Return the predecessors of a node scoped to the time specified by the head node\n",
    "    \n",
    "    arguments:\n",
    "    node -- node to return predecessors for\n",
    "    head -- node containing the timestamp to scope the graph to\n",
    "    G -- graph containing the nodes\n",
    "    \"\"\"\n",
    "    max_time = G.node[head]['date']\n",
    "    return [n for n in G.pred[node] if G.node[n]['date'] <= max_time]\n",
    "\n",
    "def scoped_neighborhood(node, head_node, G):\n",
    "    \"\"\"\n",
    "    Return the neighborhood of a node at a given timestamp\n",
    "    \n",
    "    arguments:\n",
    "    node -- node to find the neighborhood for\n",
    "    head_node -- node containing the timestamp to filter the nodes by\n",
    "    G -- directed graph containing the node\n",
    "    \"\"\"\n",
    "    max_date = G.node[head_node]['date']\n",
    "    pre = predecessors_scoped(node, head_node, G)\n",
    "    suc = successors_scoped(node, head_node, G)\n",
    "    return list(set(pre).union(set(suc)))    \n",
    "    \n",
    "def scoped_out_degree(node, head_node, G):\n",
    "    \"\"\"\n",
    "    Return the degree of a node at a given timestamp\n",
    "    \n",
    "    arguments:\n",
    "    node -- node to return the degree for\n",
    "    head_node -- node containing the timestamp to filter the nodes by\n",
    "    G -- graph containing the nodes\n",
    "    \"\"\"\n",
    "    max_date = G.node[head_node]['date']\n",
    "    neighborhood = [n for n in G[node] if G.node[n]['date'] <= max_date]\n",
    "    return len(neighborhood)\n",
    "\n",
    "def scoped_in_degree(node, head_node, G):\n",
    "    \"\"\"\n",
    "    Return the degree of a node at a given timestamp\n",
    "    \n",
    "    arguments:\n",
    "    node -- node to return the degree for\n",
    "    head_node -- node containing the timestamp to filter the nodes by\n",
    "    G -- graph containing the nodes\n",
    "    \"\"\"\n",
    "    max_date = G.node[head_node]['date']\n",
    "    neighborhood = [n for n in G.pred[node] if G.node[n]['date'] <= max_date]\n",
    "    return len(neighborhood)\n",
    "\n",
    "def scoped_degree(node, head_node, G):\n",
    "    \"\"\"\n",
    "    Return the degree of a node at a given timestamop\n",
    "    \"\"\"\n",
    "    return len(scoped_neighborhood(node, head_node, G))\n",
    "\n",
    "def get_common_neighbors(x,y,G):\n",
    "    u = set(scoped_neighborhood(x,x,G))\n",
    "    v = set(scoped_neighborhood(y,x,G))\n",
    "    return u.intersection(v)\n",
    "\n",
    "def get_neighbor_union(x,y,G):\n",
    "    u = set(scoped_neighborhood(x,x,G))\n",
    "    v = set(scoped_neighborhood(y,x,G))\n",
    "    return u.union(v)\n",
    "\n",
    "def get_common_referrers(x,y,G):\n",
    "    common_referrers_source = {n for m in successors_scoped(x, x, G) for n in predecessors_scoped(m, x, G)} - {x}\n",
    "    referrers_to_target = set(predecessors_scoped(y,x,G))\n",
    "    return common_referrers_source.intersection(referrers_to_target)\n",
    "\n",
    "    \n",
    "def common_referrers(validation_set, G, weighting_scheme=None):\n",
    "    \"\"\"\n",
    "    For a given node pair x and y, return the number of nodes that both refer to a node that x also refers and refers to y\n",
    "    or f(y) intersection g(f(x)) where f returns the predeccessors of a node and g returns the successors\n",
    "    \n",
    "    arguments:\n",
    "    validation_set -- list of edges to score\n",
    "    G -- digraph containing the nodes in the edges of the validation set\n",
    "    \n",
    "    returns:\n",
    "    list of edges with score as an attribute\n",
    "    \"\"\"\n",
    "    results = []   \n",
    "    for non_edge in validation_set:\n",
    "        x = non_edge[0]\n",
    "        y = non_edge[1]\n",
    "        cn = get_common_referrers(x,y,G)\n",
    "        if weighting_scheme and weighting_scheme.is_local():\n",
    "            weights = []\n",
    "            for z in cn:\n",
    "                weights.append(weighting_scheme.score(z, x, G) * 1.0)\n",
    "            s = sum(weights)\n",
    "        else:\n",
    "            s = len(cn)\n",
    "        if weighting_scheme and weighting_scheme.is_global():\n",
    "            s = s*weighting_scheme.score(x,y,G)\n",
    "        if len(non_edge) > 2:\n",
    "            non_edge[2]['score'] = s\n",
    "            results.append(non_edge)\n",
    "        else:\n",
    "            results.append(s)\n",
    "    return results\n",
    "\n",
    "def common_neighbors(validation_set, G, weighting_scheme=None):\n",
    "    \"\"\"\n",
    "    Perform common neighbors scoring on a list of edges\n",
    "    \n",
    "    arguments:\n",
    "    validation_set -- list of edges to score\n",
    "    G -- digraph containing the nodes in the edges of the validation set\n",
    "    \n",
    "    returns:\n",
    "    list of edges with score as an attribute\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for non_edge in validation_set:\n",
    "        x = non_edge[0]\n",
    "        y = non_edge[1]\n",
    "        cn = get_common_neighbors(x,y,G)\n",
    "        if weighting_scheme and weighting_scheme.is_local():\n",
    "            weights = []\n",
    "            for z in cn:\n",
    "                weights.append(weighting_scheme.score(z, x, G) * 1.0)\n",
    "            s = sum(weights)\n",
    "        else:\n",
    "            s = len(cn)\n",
    "        if weighting_scheme and weighting_scheme.is_global():\n",
    "            s = s*weighting_scheme.score(x,y,G)\n",
    "        if len(non_edge) > 2:\n",
    "            non_edge[2]['score'] = s\n",
    "            results.append(non_edge)\n",
    "        else:\n",
    "            results.append(s)\n",
    "    return results\n",
    "\n",
    "class WeightingScheme():\n",
    "    \n",
    "    def is_global(self):\n",
    "        return self.weight_type == \"global\"\n",
    "    def is_local(self):\n",
    "        return self.weight_type == \"local\"\n",
    "        \n",
    "\n",
    "class Jaccard(WeightingScheme):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weight_type = \"global\"\n",
    "    \n",
    "    def score(self,x,y,G):\n",
    "        \"\"\"\n",
    "        Perform jaccard scoring on a list of edges\n",
    "\n",
    "        arguments:\n",
    "        validation_set -- list of edges to score\n",
    "        G -- digraph containing the nodes in the edges of the validation set\n",
    "\n",
    "        returns:\n",
    "        list of edges with score as an attribute\n",
    "        \"\"\"\n",
    "        u = set(scoped_neighborhood(x,x,G))\n",
    "        v = set(scoped_neighborhood(y,x,G))\n",
    "        try:\n",
    "            s = 1.0*len(u & v)/len(u | v)\n",
    "        except:\n",
    "            s = 0.0\n",
    "        return s\n",
    "    \n",
    "class AdamicAdar(WeightingScheme):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weight_type = 'local'\n",
    "        \n",
    "    def score(self, z, x, G):\n",
    "        \"\"\"\n",
    "        Return the AdamicAdar coefficient for a single neighbor z, scoped to the time of a node x\n",
    "        \n",
    "        arguments:\n",
    "        z -- node to return the AA coefficient for\n",
    "        x -- node that holds the timestamp to scope z to\n",
    "        G -- graph containing x and z\n",
    "        \"\"\"\n",
    "        deg = scoped_degree(z,x,G)\n",
    "        if deg > 1:\n",
    "            return 1.0/math.log(scoped_degree(z, x, G))\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "        \n",
    "class ResourceAllocation(WeightingScheme):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weight_type = 'local'\n",
    "        \n",
    "    def score(self, z, x, G):\n",
    "        \"\"\"\n",
    "        Return the RA coefficient for a single neighbor z, scoped to the time of a node x\n",
    "        \n",
    "        arguments:\n",
    "        z -- node to return the AA coefficient for\n",
    "        x -- node that holds the timestamp to scope z to\n",
    "        G -- graph containing x and z\n",
    "        \"\"\"\n",
    "        deg = scoped_degree(z,x,G)\n",
    "        if deg > 1:\n",
    "            return 1.0/scoped_degree(z, x, G)\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "\n",
    "        \n",
    "class LeichtHolmeNewman(WeightingScheme):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weight_type = 'global'\n",
    "        \n",
    "    def score(self, x, y, G):\n",
    "        \"\"\"\n",
    "        Return the RA coefficient for a single neighbor z, scoped to the time of a node x\n",
    "        \n",
    "        arguments:\n",
    "        z -- node to return the AA coefficient for\n",
    "        x -- node that holds the timestamp to scope z to\n",
    "        G -- graph containing x and z\n",
    "        \"\"\"\n",
    "        x_deg = scoped_degree(x,x,G)\n",
    "        y_deg = scoped_degree(y,x,G)\n",
    "        if x_deg > 0 and y_deg > 0:\n",
    "            return 1.0/x_deg*y_deg\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "class HubDepressed(WeightingScheme):\n",
    "    def __init__(self):\n",
    "        self.weight_type = 'local'\n",
    "\n",
    "    def score(self, x, y, G):\n",
    "        x_deg = scoped_degree(x,x,G)\n",
    "        y_deg = scoped_degree(y,x,G)\n",
    "        if x_deg != 0 and y_deg != 0:\n",
    "            return 1.0/min(x_deg, y_deg)\n",
    "        else:\n",
    "            return 0.0\n",
    "    \n",
    "class HubPromoted(WeightingScheme):\n",
    "    def __init__(self):\n",
    "        self.weight_type = 'global'\n",
    "\n",
    "    def score(self, x, y, G):\n",
    "        x_deg = scoped_degree(x,x,G)\n",
    "        y_deg = scoped_degree(y,x,G)\n",
    "        if x_deg != 0 or y_deg != 0:\n",
    "            return 1.0/max(x_deg, y_deg)\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "class Salton(WeightingScheme):\n",
    "    def __init__(self):\n",
    "        self.weight_type = 'global'\n",
    "\n",
    "\n",
    "    def score(self, x, y, G):\n",
    "        x_deg = scoped_degree(x,x,G)\n",
    "        y_deg = scoped_degree(y,x,G)\n",
    "        if x_deg != 0 and y_deg != 0:\n",
    "            return 1.0/math.sqrt(x_deg*y_deg)\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "\n",
    "def leicht_holme_newman(validation_set, G):\n",
    "    \"\"\"\n",
    "    Perform LHN1 scoring on a single edge\n",
    "    \n",
    "    arguments:\n",
    "    non_edge -- edge tuple specified by node endpoints\n",
    "    G -- graph containing the nodes in the edge\n",
    "    \n",
    "    return:\n",
    "    edges with the score as an attribute\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for non_edge in validation_set:\n",
    "        x = non_edge[0]\n",
    "        y = non_edge[1]\n",
    "        cn = get_common_neighbors(x,y,G)\n",
    "        if scoped_out_degree(x, x, G) == 0 or scoped_in_degree(y, x, G) == 0:\n",
    "            s = 0.0\n",
    "        else:\n",
    "            s = 1.0*len(cn)/(scoped_out_degree(x, x, G)*scoped_in_degree(y, x, G))\n",
    "        if len(non_edge) > 2:\n",
    "            non_edge[2]['score'] = s\n",
    "            results.append(non_edge)\n",
    "        else:\n",
    "            results.append(s)\n",
    "    return results\n",
    "\n",
    "def adamic_adar(validation_set, G):\n",
    "    \"\"\"\n",
    "    Perform jaccard scoring on a list of edges\n",
    "    \n",
    "    arguments:\n",
    "    validation_set -- list of edges to score\n",
    "    G -- digraph containing the nodes in the edges of the validation set\n",
    "    \n",
    "    returns:\n",
    "    list of edges with score as an attribute\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for non_edge in validation_set:\n",
    "        x = non_edge[0]\n",
    "        y = non_edge[1]\n",
    "        neighbors = get_common_neighbors(x,y,G)\n",
    "        s = -1*sum([1.0/math.log(scoped_degree(node, x, G)) for node in neighbors if scoped_degree(node, x, G) > 1])\n",
    "        if len(non_edge) > 2:\n",
    "            non_edge[2]['score'] = s\n",
    "            results.append(non_edge)\n",
    "        else:\n",
    "            results.append(s)\n",
    "    return results\n",
    "\n",
    "def resource_allocation(validation_set, G):\n",
    "    \"\"\"\n",
    "    Perform resource allocation scoring on a list of edges\n",
    "    \n",
    "    arguments:\n",
    "    validation_set -- list of edges to score\n",
    "    G -- digraph containing the nodes in the edges of the validation set\n",
    "    \n",
    "    returns:\n",
    "    list of edges with score as an attribute\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for non_edge in validation_set:\n",
    "        x = non_edge[0]\n",
    "        y = non_edge[1]\n",
    "        neighbors = get_common_neighbors(x,y,G)\n",
    "        s = sum([1.0/scoped_degree(node, x, G) for node in neighbors if scoped_degree(node, x, G) > 1])\n",
    "        if len(non_edge) > 2:\n",
    "            non_edge[2]['score'] = s\n",
    "            results.append(non_edge)\n",
    "        else:\n",
    "            results.append(s)\n",
    "    return results\n",
    "\n",
    "\n",
    "def preferential_attachment(validation_set, G):\n",
    "    \"\"\"\n",
    "    Perform preferential attachment scoring on a list of edges\n",
    "    \n",
    "    arguments:\n",
    "    validation_set -- list of edges to score\n",
    "    G -- digraph containing the nodes in the edges of the validation set\n",
    "    \n",
    "    returns:\n",
    "    list of edges with score as an attribute\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for non_edge in validation_set:\n",
    "        x = non_edge[0]\n",
    "        y = non_edge[1]\n",
    "        s = scoped_out_degree(x,x,G)*scoped_in_degree(y,x,G)\n",
    "        non_edge[2]['score'] = s\n",
    "        results.append(non_edge)\n",
    "    return results\n",
    "\n",
    "def triadic_closeness(validation_set, G, census=None, weighting_scheme=None):\n",
    "    results = []\n",
    "    if census == None:\n",
    "        census = triadic_distribution(G)\n",
    "    for non_edge in validation_set:\n",
    "        x = non_edge[0]\n",
    "        y = non_edge[1]\n",
    "        u = set(scoped_neighborhood(x,x,G))\n",
    "        v = set(scoped_neighborhood(y,x,G))\n",
    "        cn = u.intersection(v)\n",
    "        t_score = []\n",
    "        for z in cn:\n",
    "            triad = get_triad(x,z,y,G)\n",
    "            if census[triad] == 0:\n",
    "                continue\n",
    "            try:\n",
    "                F2 = census[triad+30]\n",
    "            except KeyError:\n",
    "                F2 = 0.0\n",
    "            try:\n",
    "                F1 = census[triad+10]\n",
    "            except KeyError:\n",
    "                F1 = 0.0\n",
    "            \n",
    "            # Add in any weighting schemes that work on the individual nodes in the common neighborhood\n",
    "            if weighting_scheme and weighting_scheme.is_local():\n",
    "                score = weighting_scheme.score(z, x, GCC)*(1.0*F1 + F2)/census[triad]\n",
    "            else:\n",
    "                score = (1.0*F1 + F2)/census[triad]\n",
    "            \n",
    "            t_score.append(score)\n",
    "        s = sum(t_score)\n",
    "        if weighting_scheme and weighting_scheme.is_global():\n",
    "            s = s*weighting_scheme.score(x, y, G)\n",
    "        if len(non_edge) > 2:\n",
    "            non_edge[2]['score'] = s\n",
    "            results.append(non_edge)\n",
    "        else:\n",
    "            results.append(s)\n",
    "    return results\n",
    "\n",
    "def triadic_distribution(G):\n",
    "    \"\"\"\n",
    "    Return the distribution of closed triad configurations for a graph\n",
    "    The distribution is labelled as in the paper by Schall\n",
    "    http://link.springer.com.proxy.findit.dtu.dk/article/10.1007/s13278-014-0157-9\n",
    "    \n",
    "    arguments:\n",
    "    G -- directed graph\n",
    "    \n",
    "    returns:\n",
    "    Dict of labels with counts\n",
    "    \"\"\"\n",
    "    \n",
    "    # Integer labels as presented in the paper\n",
    "    TRIAD_NAMES = range(1,10) + range(11, 20) + range(21,30) + range (31, 40)\n",
    "    census = {name: 0 for name in TRIAD_NAMES}\n",
    "    for u in G.nodes_iter():\n",
    "        u_neighbors = list(set(G.successors(u)) | set(G.predecessors(u)))\n",
    "        for z in u_neighbors:\n",
    "            z_neighbors = list((set(G.successors(z)) | set(G.predecessors(z))) - {u} )\n",
    "            for v in z_neighbors:\n",
    "                name = get_triad(u,z,v, G)\n",
    "                census[name] += 1\n",
    "    return census\n",
    "\n",
    "\n",
    "def get_triad(u,z,v,G):\n",
    "    \"\"\"\n",
    "    Return the triad created by the nodes u,v and z\n",
    "    This implementation is quite probably awful.\n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    u -- starting node\n",
    "    z -- connecting node\n",
    "    v -- ending node\n",
    "    G -- DiGraph containing the nodes\n",
    "    \n",
    "    returns:\n",
    "    Dict containing closed triad counts\n",
    "    \"\"\"\n",
    "    \n",
    "    u_out = G[u]\n",
    "    v_out = G[v]\n",
    "    z_out = G[z]\n",
    "    id = 0\n",
    "    \n",
    "    if v in u_out and u in v_out:\n",
    "        id = 30\n",
    "    elif u in v_out:\n",
    "        id = 20\n",
    "    elif v in u_out:\n",
    "        id = 10\n",
    "    \n",
    "    if u in z_out and z not in u_out:\n",
    "        id += 7\n",
    "        if v in z_out and z not in v_out:\n",
    "            return id + 2\n",
    "        elif z in v_out and v not in z_out:\n",
    "            return id + 1\n",
    "        elif z in v_out and v in z_out:\n",
    "            return id\n",
    "        raise Exception(\"Error in finding triad\")\n",
    "    elif z in v_out and v not in z_out:\n",
    "        id += 5\n",
    "        if z in u_out and u not in z_out:\n",
    "            return id + 1\n",
    "        elif z in u_out and u in z_out:\n",
    "            return id\n",
    "        raise Exception(\"Error in finding triad\")\n",
    "    elif z in u_out and u not in z_out:\n",
    "        id += 3\n",
    "        if v in z_out and z not in v_out:\n",
    "            return id + 1\n",
    "        if v in z_out and z in v_out:\n",
    "            return id\n",
    "        raise Exception(\"Error in finding triad\")\n",
    "    elif z in u_out and u in z_out:\n",
    "        if v in z_out and z not in v_out:\n",
    "            return id + 2\n",
    "        elif v in z_out and z in v_out:\n",
    "            return id + 1\n",
    "    \n",
    "    raise Exception(\"No triad found\")\n",
    "        \n",
    "\n",
    "def get_closed_triads(x, y, G):\n",
    "    \"\"\"\n",
    "    Return the closed triads generated by adding a link from x to y.\n",
    "    Triads are classified according to the triadic_census algorithm of NetworkX based on \n",
    "    http://vlado.fmf.uni-lj.si/pub/networks/doc/triads/triads.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    # Taken directly from nx.triadic_census source\n",
    "    TRIAD_NAMES = ('003', '012', '102', '021D', '021U', '021C', '111D', '111U',\n",
    "                   '030T', '030C', '201', '120D', '120U', '120C', '210', '300')\n",
    "    census = {name: 0 for name in TRIAD_NAMES}\n",
    "    \n",
    "    x_in = set(G.predecessors(x))\n",
    "    y_in = set(G.predecessors(y))\n",
    "    x_out = set(G.successors(x))\n",
    "    y_out = set(G.successors(y))\n",
    "    \n",
    "    for node in x_in | y_in | x_out | y_out:\n",
    "        # y refers to a node that refers to x\n",
    "        if node in x_in and node in y_out:\n",
    "            census['030C'] += 1\n",
    "        # y is being referred to by a node that refers to x\n",
    "        if node in x_in and node in y_in:\n",
    "            census['030T'] += 1\n",
    "        # x refers to a node that refers to y\n",
    "        if node in x_out and node in y_in:\n",
    "            census['030T'] += 1\n",
    "        # x refers to a node that y refers to\n",
    "        if node in x_out and node in y_out:\n",
    "            census['030T'] += 1\n",
    "\n",
    "def time_difference(G, candidate_edges):\n",
    "    \"\"\"\n",
    "    Return the time difference for a set of edges\n",
    "    \"\"\"\n",
    "    return [int((G.node[x]['date'] - G.node[y]['date']).days) for x,y in candidate_edges]\n",
    "        \n",
    "class CommonNeighbors():\n",
    "    \"\"\"\n",
    "    An experiment that scores a list of edges based on the number of common neighbors between the source and the target node\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, weighting_scheme=None):\n",
    "        self.weighting_scheme = weighting_scheme\n",
    "        \n",
    "    def valid_false_edges(self, G, source_nodes):\n",
    "        \"\"\"\n",
    "        Returns a list of edges that do not exist in G and will score highly on the common neighbors index\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        G: networkX.Graph\n",
    "            Graph to perform experiment on\n",
    "        source_nodes: list of tuples\n",
    "            Nodes containing the true edges to be scored\n",
    "        \"\"\"\n",
    "        false_validation_edges = [(source, target) for source in source_nodes\n",
    "                   for neighbor in G[source].keys()\n",
    "                   for target in set(scoped_neighborhood(neighbor, source, G)) - {source} if target not in G[source].keys()]\n",
    "        \n",
    "        false_validation_edges = list(set(false_validation_edges))\n",
    "        for i in range(len(false_validation_edges)):\n",
    "            edge = false_validation_edges[i]\n",
    "            # Add date information to the non-edge\n",
    "            false_validation_edges[i] = edge + ({'date': G.node[edge[0]]['date']},)\n",
    "        \n",
    "        return false_validation_edges\n",
    "        \n",
    "    def score_edges(self, edges, G):\n",
    "        return common_neighbors(edges, G, self.weighting_scheme)\n",
    "    \n",
    "class TriadicCloseness(CommonNeighbors):\n",
    "    \n",
    "    def __init__(self, cache_distribution=False, weighting_scheme=None):\n",
    "        self.cache_distribution = cache_distribution\n",
    "        self.census = None\n",
    "        self.weighting_scheme=weighting_scheme\n",
    "    \n",
    "    def score_edges(self, edges, G):\n",
    "        \n",
    "        if self.cache_distribution == True and self.census == None:\n",
    "            self.census = triadic_distribution(G)\n",
    "            return triadic_closeness(edges, G, self.census, self.weighting_scheme)\n",
    "        else:\n",
    "            return triadic_closeness(edges, G, self.census, self.weighting_scheme)\n",
    "    \n",
    "class CommonReferrers():\n",
    "    \n",
    "    def __init__(self, weighting_scheme=None):\n",
    "        self.weighting_scheme = weighting_scheme\n",
    "    \n",
    "    def valid_false_edges(self, G, source_nodes):\n",
    "        \"\"\"\n",
    "        Returns a list of edges that do not exist in G and will score highly on the common referrers index\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        G: networkX.Graph\n",
    "            Graph to perform experiment on\n",
    "        source_nodes: list of tuples\n",
    "            Nodes containing the true edges to be scored\n",
    "        \"\"\"\n",
    "        non_edges = []\n",
    "        for source in source_nodes:\n",
    "            neighbors = G[source].keys()\n",
    "            corefs = set()\n",
    "            for neighbor in neighbors:\n",
    "                corefs = corefs | {c for c in predecessors_scoped(neighbor, source, G)}\n",
    "            [non_edges.append((source, referee)) for coref in (corefs - {source}) \n",
    "                                                 for referee in set(successors_scoped(coref, source, G)) - set(neighbors)]\n",
    "            \n",
    "        for i in range(len(non_edges)):\n",
    "            edge = non_edges[i]\n",
    "            # Add date information to the non-edge\n",
    "            non_edges[i] = edge + ({'date': G.node[edge[0]]['date']},)\n",
    "        \n",
    "        return non_edges\n",
    "    \n",
    "    def score_edges(self, edges, G):\n",
    "        return common_referrers(edges, G, self.weighting_scheme)\n",
    "            \n",
    "                \n",
    "def valid_random_non_edges(graph, n):\n",
    "    \"\"\"\n",
    "    Returns randomized, non-existent links between nodes in the graph that are guaranteed to observe causality.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph : NetworkX graph.\n",
    "        Graph to find non-existent edges.\n",
    "    n : integer\n",
    "        Number of non-existent edges to find\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    non_edges : list\n",
    "        List of n edges that are not in the graph.\n",
    "    \"\"\"\n",
    "    result_pairs = {}\n",
    "    # Sort edges according to age\n",
    "    sorted_edges =[node for node, data in sorted(graph.nodes(data=True), key=lambda x: x[1]['date'], reverse=True)]\n",
    "    node_set = set(graph.nodes())\n",
    "    candidates = list(np.random.choice(sorted_edges, n, replace=True))\n",
    "    i = 0\n",
    "    while i < len(candidates):\n",
    "        u = candidates[i]\n",
    "        if not u in result_pairs.keys():\n",
    "            result_pairs[u] = []\n",
    "            \n",
    "        # Make sure the potential neighbors respect causality with a resolution equal to the timestamp\n",
    "        cand_index = sorted_edges.index(u)\n",
    "        potential_neighbors = set(sorted_edges[cand_index:])\n",
    "        if graph.is_directed():\n",
    "            neighbors = set(graph.successors(u)).union(set(graph.predecessors(u)))\n",
    "        else:\n",
    "            neighbors = set(graph.neighbors(u))\n",
    "        # Make sure the potential neighbors respect causality\n",
    "        non_neighbors = list(potential_neighbors - neighbors - set(result_pairs[u]))\n",
    "        # The oldest node will have a neighborhood of Ã˜, so add a new candidate to the list in that case\n",
    "        if len(non_neighbors) == 0:\n",
    "            candidates.append(random.choice(graph.nodes()))\n",
    "        else:    \n",
    "            result_pairs[u].append(random.choice(non_neighbors))\n",
    "        i += 1\n",
    "    \n",
    "    return [(k,v) for k, arr in result_pairs.iteritems() for v in arr]\n",
    "        \n",
    "\n",
    "def k_fold_validate(G, k, experiment):\n",
    "    \"\"\"\n",
    "    K-fold validation of some specified function\n",
    "    \n",
    "    arguments:\n",
    "    G -- Graph to perform the function on\n",
    "    k -- number of folds\n",
    "    fun -- function to be evaluated\n",
    "    kwargs -- arguments to be passed to the evaluated function\n",
    "    \n",
    "    return:\n",
    "    List of lists of scored predictions\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    validation_sets = []\n",
    "    edges = G.edges(data=True)\n",
    "    random.shuffle(edges)\n",
    "    # Find all edges that do not exist in the graph, but will return a high score\n",
    "    false_edges = experiment.valid_false_edges(G, G.nodes())\n",
    "    random.shuffle(false_edges)\n",
    "    M = len(false_edges)/k\n",
    "    # Find the number of true members in the validation set\n",
    "    N = len(edges)/k\n",
    "    for i in range(0,k):\n",
    "        validation_sets.append(edges[i*N:(i+1)*N] + false_edges[i*M:(i+1)*M])\n",
    "    for val_edges in validation_sets:\n",
    "        res = experiment.score_edges(val_edges, G)\n",
    "        # If not shuffled, subsequent sorting algorithms will always rank true edges higher than false edges when they have\n",
    "        # the same score\n",
    "        random.shuffle(res)\n",
    "        results.append(res)\n",
    "    return results\n",
    "\n",
    "def k_fold_nodes(G, k, experiment):\n",
    "    \"\"\"\n",
    "    Perform k-fold validation on the nodes of the graph.\n",
    "    This validation only tests relevant false edges, i.e. edges where we are fairly certain the score won't come out as 0.0\n",
    "    making it useful for precision / recall, but not for AUC. The true edges can still score 0.0\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    nodes = G.nodes()\n",
    "    random.shuffle(nodes)\n",
    "    N = len(nodes)/k\n",
    "    validation_sets = []\n",
    "    for i in range(0, k):\n",
    "        validation_sets.append(nodes[i*N:(i+1)*N])\n",
    "    for true_nodes in validation_sets:\n",
    "        # Validation sets consist of all possible the edges in \n",
    "        val_edges = [(source, target, G[source][target]) for source in true_nodes for target in G[source].keys()]\n",
    "        val_set = set((source, target) for (source, target, _) in val_edges)\n",
    "        G_train = G.copy()\n",
    "        G_train.remove_edges_from(val_edges)\n",
    "        false_validation_edges = experiment.valid_false_edges(G, true_nodes)\n",
    "        false_validation_edges = [(x,y,data) for (x,y,data) in false_validation_edges if (x,y) not in val_set]\n",
    "        validation_set = val_edges + false_validation_edges\n",
    "        # If not shuffled, subsequent sorting algorithms will always rank true edges higher than false edges when they have\n",
    "        # the same score\n",
    "        random.shuffle(validation_set)        \n",
    "        results.append(experiment.score_edges(validation_set, G))\n",
    "    return results\n",
    "\n",
    "def at_degree_validation(G, experiment, degree):\n",
    "    if degree < 2:\n",
    "        raise Exception('Degree must be larger than 2')\n",
    "    res = {}\n",
    "    H = G.copy()\n",
    "    to_be_evaluated = []\n",
    "    # Randomly sample n nodes from each degree bin\n",
    "    deg=nx.degree(G)\n",
    "    h = {}\n",
    "    for node in G.nodes_iter():\n",
    "        if G.out_degree(node) == degree:\n",
    "            to_be_evaluated.append(node)\n",
    "    for node in to_be_evaluated:\n",
    "        res[node] = []\n",
    "        e = G.edges(node, data=True)\n",
    "        if not len(e) == degree:\n",
    "            raise Exception(\"Mismatch: {}\".format(len(e)))\n",
    "        G.remove_edges_from(e[1:degree-1])\n",
    "        for i in range(1, degree):\n",
    "            to_be_scored = experiment.valid_false_edges(H, [node]) + e[i:degree]\n",
    "            random.shuffle(to_be_scored)\n",
    "            scored_edges = experiment.score_edges(to_be_scored, G)\n",
    "            [edge[2].update({'n_edges': i}) for edge in scored_edges]\n",
    "            res[node] += scored_edges\n",
    "            #res[node][i] = scored_edges\n",
    "            G.add_edge(e[i][0], e[i][1])\n",
    "    return res\n",
    "        \n",
    "def precision(G, results, L):\n",
    "    \"\"\"\n",
    "    Find the ratio of the true positives to trues\n",
    "    \n",
    "    arguments:\n",
    "    G -- graph the results are based on\n",
    "    results -- list of lists of scored predictions\n",
    "    L -- number of results to be considered\n",
    "    \n",
    "    return:\n",
    "    List of precisions for each set of results\n",
    "    \"\"\"\n",
    "    # Sort the results with descending scores\n",
    "    results = [sorted(result, key=lambda x: x[2]['score'], reverse=True) for result in results]\n",
    "    for res in results:\n",
    "        if L > len(res):\n",
    "            raise ArgumentError(\"L is larger than the number of results\")\n",
    "    edge_set = set(G.edges())\n",
    "    # True positives exist in both the edge set and the result set\n",
    "    true_positives = [[(edge[0],edge[1]) for edge in result[0:L] if (edge[0], edge[1]) in edge_set] for result in results]\n",
    "    return [1.0*len(trues)/L for trues in true_positives]\n",
    "\n",
    "def per_node_precision(G, results):\n",
    "    \"\"\"\n",
    "    results : dict\n",
    "        A dict of {node: [e1, e2, ... e_n]} where e is an edge of the form (source, target, {'n_edges': x, 'score': y}) where\n",
    "        n_edges number of edges the node had when the prediction was made.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = 43\n",
    "    # Create a dict that all the edges sorted by score and grouped by n_edges and then node\n",
    "    s = {n: {k: [] for k in range(1,m+1)} for n in results.iterkeys()}\n",
    "    for node, node_res in results.iteritems():\n",
    "        # Sort the entire list of results\n",
    "        srt = sorted(node_res, key=lambda x: x[2]['score'], reverse=True)\n",
    "        for x,y,data in srt:\n",
    "            # Add each result to its proper bin\n",
    "            s[node][data['n_edges']].append((x,y,{'score': data['score']}))\n",
    "    precisions = {k: [] for k in range(1,m+1)}\n",
    "    edge_set = set(G.edges())\n",
    "    for node, node_res in s.iteritems():\n",
    "        n = G.out_degree(node)\n",
    "        for level, results in node_res.iteritems():\n",
    "            L = n-level\n",
    "            if len(results) == 0:\n",
    "                break\n",
    "            precisions[level].append(1.0*len([(edge[0],edge[1]) for edge in results[0:L] if (edge[0], edge[1]) in edge_set])/L)\n",
    "    return precisions\n",
    "    \n",
    "\n",
    "def precision2(G, results, L):\n",
    "    \"\"\"\n",
    "    Find the ratio of the true positives to trues\n",
    "    \n",
    "    arguments:\n",
    "    G -- graph the results are based on\n",
    "    results -- list of lists of scored predictions\n",
    "    L -- number of results to be considered\n",
    "    \n",
    "    return:\n",
    "    List of precisions for each set of results\n",
    "    \"\"\"\n",
    "    # Sort the results with descending scores\n",
    "    #results = heapq.nlargest(L, results, key=lambda x: x[2]['score'])\n",
    "    edge_set = set(G.edges())\n",
    "    # True positives exist in both the edge set and the result set\n",
    "    true_positives = [(edge[0],edge[1]) for edge in results if (edge[0], edge[1]) in edge_set]\n",
    "    return 1.0*len(true_positives)/L\n",
    "\n",
    "def AUC(G, results):\n",
    "    \"\"\"\n",
    "    Perform n trials where the score of a non-edge and an edge in the result is compared. Count the number of trials where\n",
    "    the edge had the higher score as n' and the number of times the score was equal as n'' and return the AUC as (n' + n'')/n.\n",
    "    \n",
    "    arguments:\n",
    "    G -- graph the results are based on\n",
    "    results -- list of lists of scored predictions\n",
    "    \n",
    "    return:\n",
    "    List of precisions for each set of results\n",
    "    \"\"\"\n",
    "    \n",
    "    edge_set = set(G.edges())\n",
    "    AUC = []\n",
    "    for result_set in results:\n",
    "        true_edges = []\n",
    "        false_edges = []\n",
    "        for (x,y,data) in result_set:\n",
    "            if (x,y) in edge_set:\n",
    "                true_edges.append((x,y,data))\n",
    "            else:\n",
    "                false_edges.append((x,y,data))\n",
    "        \n",
    "        random.shuffle(true_edges)\n",
    "        random.shuffle(false_edges)\n",
    "        n = len(true_edges)\n",
    "        n_better = 0.0\n",
    "        n_same = 0.0\n",
    "        for i in range(0, n):\n",
    "            if true_edges[i][2]['score'] > false_edges[i][2]['score']:\n",
    "                n_better += 1.0\n",
    "            if true_edges[i][2]['score'] == false_edges[i][2]['score']:\n",
    "                n_same += 1.0\n",
    "        AUC.append((n_better + 0.5*n_same)/n)\n",
    "    return AUC\n",
    "\n",
    "def slice_graph_by_year(start_year, end_year, G):\n",
    "    \"\"\"\n",
    "    Returns all nodes created within a range of years\n",
    "    \"\"\"\n",
    "    if datetime.date(start_year, 1, 1) < datetime.date(end_year, 1, 1):\n",
    "        raise ArgumentError(\"The starting year must be larger than or equal to the ending year\")\n",
    "    t1 = datetime.date(start_year, 12, 31)\n",
    "    t2 = datetime.date(end_year, 1, 1)\n",
    "    \n",
    "    return [n for n in G.nodes(data=True) if n[1]['date'] >= t2 and n[1]['date'] < t1]\n",
    "\n",
    "f = lambda x, y, G: y in G[x].keys()\n",
    "features = ['edge',\n",
    "            'triadic_closeness',\n",
    "            'common_neighbors',\n",
    "            'time_difference',\n",
    "            'common_referrers',\n",
    "            'src_degree', \n",
    "            'trg_degree', \n",
    "            'degree_product',\n",
    "            'common_referrersadamic_adar',\n",
    "            'adamic_adar', \n",
    "            'leicht_holme_newman',\n",
    "            'resource_allocation']\n",
    "\n",
    "def get_features(G, cand_edges, sp_path):\n",
    "    \n",
    "    longest_possible_path = len(G.nodes())\n",
    "    \n",
    "    def shortest_dag_path(G, x, y):\n",
    "        if df.loc[[(x,y)],'edge'].item() == True:\n",
    "            G.remove_edge(x,y)\n",
    "        try:\n",
    "            sp = nx.shortest_path_length(G,x,y)\n",
    "        except:\n",
    "            sp = longest_possible_path\n",
    "        if df.loc[[(x,y)],'edge'].item() == True:\n",
    "            G.add_edge(x,y)\n",
    "        return sp\n",
    "    \n",
    "    df = pd.DataFrame(np.zeros((len(cand_edges), len(features))), index=cand_edges, columns=features)\n",
    "    true_edges = set(G.edges())\n",
    "    df['edge'] = [True if edge in true_edges else False for edge in cand_edges]\n",
    "    df['triadic_closeness'] = triadic_closeness(cand_edges, G)\n",
    "    df['common_neighbors'] = common_neighbors(cand_edges, G)\n",
    "    df['time_difference'] = time_difference(G, cand_edges)\n",
    "    df['common_referrers'] = common_referrers(cand_edges, G)\n",
    "    #df['shortest_path'] = [shortest_dag_path(G,x,y) for x,y in cand_edges]\n",
    "    #df['shortest_path'] = pickle.load(open(sp_path, 'rb'))\n",
    "    df['src_degree'] = [len(G[x]) for x, _ in cand_edges]\n",
    "    df['trg_degree'] = [len(G[y]) for _, y in cand_edges]\n",
    "    df['degree_product'] = [len(G[x])*len(G[y]) for x,y in cand_edges]\n",
    "    df['adamic_adar'] = adamic_adar(cand_edges, G)\n",
    "    df['leicht_holme_newman'] = leicht_holme_newman(cand_edges, G)\n",
    "    df['resource_allocation'] = resource_allocation(cand_edges, G)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "The large dataset makes prediction times very long and the class imbalance biases heavily toward predicting the negative class.\n",
    "\n",
    "### Class imbalance and data set size\n",
    "Since there are $V^2$ possible links between vertices and only a small part of them will turn out to be actual true links the dataset suffers highly from class imbalance, which for some classifiers can lead to solutions that are biased to predicting the majority class while others are perfectly able to handle imbalanced classes without bias.\n",
    "\n",
    "If it presents a problem class imbalance can be handled by\n",
    "* Weigthing the different classes to ensure that the minority class is given more importance.\n",
    "* Undersampling the majority class with replacement. This has the benefit of lower training and feature generation times, however important information in the majority class can be missed, leading to low classifier performance on the test set.\n",
    "* Upsampling the minority class with replacement. This method does not run the risk of missing information in the training set, but bias can still be introduced and computation time increases.\n",
    "\n",
    "These methods can be combined with the bootstrap technique where several classifiers are trained on different samples and predictions on the test set is a decided by majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Prepare a single year test set\n",
    "test_year = 2013\n",
    "nodes_test = [n[0] for n in slice_graph_by_year(test_year, test_year, GCC)]\n",
    "G_train = GCC.copy()\n",
    "G_train.remove_edges_from(nodes_test)\n",
    "test_candidates = [(source, target) for target in GCC.nodes() for source in nodes_test if not source==target]\n",
    "test_data = get_features(GCC, test_candidates, 'pickles/sp_test_2013.pkl')\n",
    "y_test = test_data['edge']\n",
    "X_test = test_data.ix[:, test_data.columns != 'edge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Prepare a multi-year training set, this is the full set of edges which will take a very long time to compute\n",
    "train_year = 2012\n",
    "nodes_train = [n[0] for n in slice_graph_by_year(train_year, train_year, G_train)]\n",
    "train_candidates = [(source, target) for target in G_train.nodes() for source in nodes_train if not source==target]\n",
    "train_data = get_features(GCC, train_candidates, 'pickles/sp_train_2013.pkl')\n",
    "y_train = train_data['edge']\n",
    "X_train = train_data.ix[:, train_data.columns != 'edge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ensure that there is no overlap between test and train set\n",
    "assert len(set(train_candidates).intersection(set(test_candidates))) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Methods\n",
    "\n",
    "Like in un-supervised learning, supervised learning in link prediction means finding a model that is able to predict whether two nodes will form a link based on other features of the network. The difference between the two is that supervised learning finds a model that explains the links already formed in a subset of the data, $G_{train}$ by combining different features such as common neighbors, node degree and node centrality and assign a weight to each feature according to its importance. This model can then be tested on the remaining data, $G_{test}$, and since this part of the network wasn't used to create the model it will give an unbiased result which can be used for evaluation.\n",
    "\n",
    "## Feature selection\n",
    "For the supervised model it is possible to use all the different features described earlier, such as Common Neighbors, Common Referrers and so on. In many cases however the features are quite similar, for example Adamic-Adar and Resource Allocation only differ in that Adamic-Adar takes the log of the denominator. Precision wise this is not necessarily a problem since the number of samples still is much greater than the number of features, but redundant features makes the model harder to interpret and if real-time results are expected in an implementation of the classifier the additional computational time might be noticable by the end user. \n",
    "For these reasons it is interesting to look at reducing the number of features used in the final model for which there are three general approaches (Hasties)\n",
    "* Subset selection which involves finding the smallest subset of features which leads to the model with the best predictive power. For $p$ features this means fitting $2^p$ models, so often the variant forward stepwise selection is used, leading to much smaller computational times (Hasties).\n",
    "* Feature shrinking involves adding a regularization term to the computation of the minimal Residual Sum of Squares (RSS) which reduces overfitting of the model and if L1-regularization is used it can even prune out redundant features.\n",
    "* Dimensionality reduction covers matrix factorization techniques such as Principal Component Analysis (PCA) and Independent Component Analysis. Both have the advantage of decreasing the feature space however the model is often harder to interpret since the original feature space is transformed and for this reason they are not considered for feature selection for this project.\n",
    "\n",
    "To identify either the best subset of features or the regularization parameter it is necessary to use cross validation or an adjusted RSS that takes model complexity into account like the Akaike Information Criterion (AIC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "In machine learning terms link prediction is a categorical problem since we are investigating the binary response to $x$ links to $y$ shown below\n",
    "$$\n",
    "    f(x,y)= \n",
    "\\begin{cases}\n",
    "    1,& \\text{if } x \\ \\text{links to} \\ y \\\\\n",
    "    0,              & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "There are many methods specifically suited to categorical variables in machine learning where logistic regression is among the simplest. It is comparable to linear regression which is used and suited for continuous variables, but instead of modeling a continuous response it models a probability of a response variable taking some discrete value, $P(\\text{link}_{x,y} \\ | \\ \\text{feature}_{x,y})$ (REF HASTIES). Since true probabilites fall between 0 and 1 the logistic function is used, which for a range of input variables returns an S-shaped curve asymptotically approaching 0 and 1 in either limit\n",
    "$$\n",
    "p(X) = \\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}\n",
    "$$\n",
    "where $\\beta$ is the trainable parameter and X is the input series. The output probability can then be evaluated to determine if a link is formed.\n",
    "Since link prediction can benefit from using several input variables instead of only one it is an advantage that logistic regression can be expanded to multiple logistic regression quite simply by simply adding an addtional $\\beta$ parameter for each additional input. As with single regression the parameters are trained using maximum likelihood methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Logistic Regression\n",
    "# Find hyper-parameters for the regressors\n",
    "# Try both L1 and L2 regularization with different coefficients\n",
    "param_grid = {'C': [10, 1000, 100000], 'penalty': ['l1', 'l2']}\n",
    "def scorer(ground_truth, predictions):\n",
    "    return average_precision_score(ground_truth, predictions[:,1])\n",
    "\n",
    "print(\"# Tuning hyper-parameters for AUC-PR\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    clf = pickle.load(open('pickles/log_grid_search.pkl', 'rb'))\n",
    "except:\n",
    "    # Do an exhaustive search of the entire grid\n",
    "    clf = GridSearchCV(LogisticRegression(), param_grid, cv=5,\n",
    "                       scoring=make_scorer(scorer, needs_proba=True))\n",
    "    clf.fit(X_train, y_train)\n",
    "    with open('pickles/log_grid_search.pkl', 'wb') as fl:\n",
    "        pickle.dump(clf, fl)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "for params, mean_score, scores in clf.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean_score, scores.std() * 2, params))\n",
    "print()\n",
    "\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "print(\"The model is trained on the full development set.\")\n",
    "print(\"The scores are computed on the full evaluation set.\")\n",
    "print()\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()\n",
    "print \"Running time: {}\".format(t1-t0)\n",
    "logistic_regression_params = clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of old model: (3000550, 11)\n",
      "Dimensions of model transformed after L1 regularization: (3000550L, 10L)\n"
     ]
    }
   ],
   "source": [
    "new_model = SelectFromModel(clf.best_estimator_, prefit=True)\n",
    "X_new = new_model.transform(X_train)\n",
    "print \"Dimensions of old model: {}\".format(X_train.shape)\n",
    "print \"Dimensions of model transformed after L1 regularization: {}\".format(X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoMAAAEZCAYAAADsey82AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x691a0668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_model = LogisticRegression(C=10, penalty='l1')\n",
    "log_model.fit(X_train, y_train)\n",
    "preds = log_model.predict_proba(X_test)\n",
    "mean_avg_precision = average_precision_score(y_test, preds[:,1])\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, preds[:,1])\n",
    "plt.plot(recall, precision, lw=1, label='Avg. preceision {}'.format(mean_avg_precision))\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision and Recall for logistic regression')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      1.00      1.00   3511712\n",
      "       True       0.82      0.55      0.66      3218\n",
      "\n",
      "avg / total       1.00      1.00      1.00   3514930\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(y_test, log_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common_neighbors: -2.23329774417\n",
      "time_difference: -0.000214402646993\n",
      "triadic_closeness: 7.25223817186\n",
      "src_degree: 0.0369196712702\n",
      "trg_degree: 0.00872682428194\n",
      "degree_product: -0.00125626567851\n",
      "common_referrersadamic_adar: 0.0\n",
      "leicht_holme_newman: 8.73508893269\n",
      "resource_allocation: 22.4355413704\n",
      "common_referrers: 0.505605004564\n",
      "adamic_adar: -0.130524422337\n"
     ]
    }
   ],
   "source": [
    "for name, coef in zip(X_train.columns.values, log_model.coef_[0]):\n",
    "    print '{}: {}'.format(name, coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees\n",
    "A decision tree is a rule based classifier which can be seen as determining the class of a sample by asking questions to the features such as, *\"If there are more than 4 common neighbors, classify it is as a link\"* and combining these rules to divide the feature space into regions of different classes. Normally these rules are created by a top-down algorithm which greedily splits the training data based on what split gives the largest decrease in RSS after which the same procedure is recursively applied to the two new partitions, ad nauseam until some stopping criterion is reached.\n",
    "\n",
    "This leads to a set of rules that will be very good at predicting the training set, however it has a high likelihood of overfitting so usually this larger tree is pruned down in a manner similar to regularization in general linear models which reduces variance at the cost of some bias in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_selection import RFECV\n",
    "clf = RandomForestClassifier()\n",
    "rfe_forest = RFECV(estimator=clf, step=1, scoring=make_scorer(scorer, needs_proba=True))\n",
    "rfe_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common_neighbors: False\n",
      "time_difference: True\n",
      "triadic_closeness: True\n",
      "src_degree: True\n",
      "trg_degree: True\n",
      "degree_product: True\n",
      "common_referrersadamic_adar: False\n",
      "leicht_holme_newman: True\n",
      "resource_allocation: True\n",
      "common_referrers: True\n",
      "adamic_adar: True\n"
     ]
    }
   ],
   "source": [
    "for name, state in zip(X_train.columns.values, rfe_forest.support_):\n",
    "    print '{}: {}'.format(name, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      1.00      1.00   3511712\n",
      "       True       0.95      0.82      0.88      3218\n",
      "\n",
      "avg / total       1.00      1.00      1.00   3514930\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(y_test, rfe_forest.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_difference: Included\n",
      "triadic_closeness: Included\n",
      "src_degree: Included\n",
      "trg_degree: Included\n",
      "degree_product: Included\n",
      "leicht_holme_newman: Included\n",
      "resource_allocation: Included\n",
      "common_referrers: Included\n",
      "adamic_adar: Included\n",
      "common_neighbors: Excluded\n",
      "common_referrersadamic_adar: Excluded\n"
     ]
    }
   ],
   "source": [
    "in_out = ['Excluded', 'Included']\n",
    "for name, in_model in sorted(zip(X_train.columns.values, rfe_forest.get_support()), key=lambda tup: tup[1], reverse=True):\n",
    "    print name + \": {}\".format(in_out[in_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoMAAAEZCAYAAADsey82AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBvv/z28DM9Zal7kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28da8358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = rfe.predict_proba(X_test)\n",
    "mean_avg_precision = average_precision_score(y_test, preds[:,1])\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, preds[:,1])\n",
    "plt.plot(recall, precision, lw=1, label='Avg. preceision {}'.format(mean_avg_precision))\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision and Recall for logistic regression')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoMAAAEZCAYAAADsey82AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nksOUDIqIiIjkMCWDIiIiIjns/wHZ6cjhxroTPgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x839e6f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common_neighbors: Included\n",
      "time_difference: Excluded\n",
      "triadic_closeness: Included\n",
      "src_degree: Excluded\n",
      "trg_degree: Excluded\n",
      "degree_product: Excluded\n",
      "common_referrersadamic_adar: Excluded\n",
      "leicht_holme_newman: Excluded\n",
      "resource_allocation: Excluded\n",
      "common_referrers: Excluded\n",
      "adamic_adar: Excluded\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      1.00      1.00   3511712\n",
      "       True       1.00      0.79      0.88      3218\n",
      "\n",
      "avg / total       1.00      1.00      1.00   3514930\n",
      "\n",
      "Wall time: 4min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rfe_tree = RFECV(estimator=tree.DecisionTreeClassifier(), step=1, scoring=make_scorer(scorer, needs_proba=True))\n",
    "rfe_tree.fit(X_train, y_train)\n",
    "preds = rfe_tree.predict_proba(X_test)\n",
    "mean_avg_precision = average_precision_score(y_test, preds[:,1])\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, preds[:,1])\n",
    "plt.plot(recall, precision, lw=1, label='Avg. preceision {}'.format(mean_avg_precision))\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision and Recall for logistic regression')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()\n",
    "in_out = ['Excluded', 'Included']\n",
    "for name, in_model in zip(X_train.columns.values, rfe_tree.get_support()):\n",
    "    print name + \": {}\".format(in_out[in_model])\n",
    "print\n",
    "print classification_report(y_test, rfe_tree.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export the tree\n",
    "dot_data = StringIO()\n",
    "tree.export_graphviz(rfe_tree.estimator_, out_file=dot_data,  \n",
    "                         feature_names=X_train.columns.values[rfe_tree.support_],  \n",
    "                         class_names=['No link', 'Link'],  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) \n",
    "graph.write_pdf(\"rfe_tree.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=20)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict_proba(X_test)\n",
    "mean_avg_precision = average_precision_score(y_test, preds[:,1])\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, preds[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAAEZCAYAAAAHTJQ1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEia+f+nA4m1r45aXQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x596b5668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(recall, precision, lw=1, label='Avg. precision: {}'.format(mean_avg_precision))\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision and Recall for logistic regression')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals.six import StringIO\n",
    "import pydotplus\n",
    "dot_data = StringIO()\n",
    "tree.export_graphviz(cl, out_file=dot_data,  \n",
    "                         feature_names=X_train.columns.values,  \n",
    "                         class_names=['No link', 'Link'],  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) \n",
    "graph.write_pdf(\"iris.pdf\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['common_neighbors', 'time_difference', 'triadic_closeness',\n",
       "       'src_degree', 'trg_degree', 'degree_product',\n",
       "       'common_referrersadamic_adar', 'leicht_holme_newman',\n",
       "       'resource_allocation', 'common_referrers', 'adamic_adar'], dtype=object)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common_neighbors: 0.00889713719616\n",
      "time_difference: 0.0768312863851\n",
      "triadic_closeness: 0.694534277001\n",
      "src_degree: 0.0207787954437\n",
      "trg_degree: 0.0179133826347\n",
      "degree_product: 0.0207242151124\n",
      "common_referrersadamic_adar: 0.0\n",
      "leicht_holme_newman: 0.0767119951329\n",
      "resource_allocation: 0.00222414592383\n",
      "common_referrers: 0.0701771102114\n",
      "adamic_adar: 0.0112076549589\n"
     ]
    }
   ],
   "source": [
    "for name, feat in zip(X_train.columns.values, cl.feature_importances_):\n",
    "    print name + \": {}\".format(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      1.00      1.00   3511712\n",
      "       True       0.94      0.82      0.88      3218\n",
      "\n",
      "avg / total       1.00      1.00      1.00   3514930\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precsion: 0.412919549455\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAEZCAYAAACq4U7ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XeYVOXZx/HvvbuAgIsiCAJSLIgVDCoBUVnBV5Aigh2U\n2LChkRhfwJgICKLG8mqsEREBpagYwRoMYdUoRjQKiNKkSItUAQWR8rx/PId1WLfMwsycM7O/z3XN\ntafNOffZmd2556nmnENEREQkDFlhByAiIiLllxIRERERCY0SEREREQmNEhEREREJjRIRERERCY0S\nEREREQmNEpE0ZGZfmNkZpRxT38w2mZmlKq69YWYjzeyusOOIZWbTzOyqYPk3ZvZ+Ccd2M7Nvgt91\ns0Reex/Pc7uZPb2Xz91sZo32NYaoS5e/EZFMp0QkgcxsiZltCf65rQo+ZKsk+jrOueOdc++Vcswy\n51w1l8YDxQRJwI7g9/mdmX1mZp1CCKWk3+H9wI3B73pmqgIqjXPuHufctaUdV1Ti45zLdc4tSVpw\nEZEJfyMimUCJSGI5oJNzrhrQHDgZ+GNRB+pbWNw+DD4sDgSeBMabWbWwg4rREPhyb55oZhn792dm\n2VE+n4hER8b+IwyRATjnVgFvAcdDwTfPoWb2LzP7ATjMzKqZ2QgzW2lmy8xsSGyCYma9zezLoETg\nCzM7Mdi+2MzaBsunmNkMM9sYlMI8EGxvaGa7dn/YmVkdM5tkZuvMbL6ZXRNznYFmNsHMRgXXmm1m\nzYu9QbOHg+qIjcG1T4v3XGb2KzP7NHjueGC/MvxuxwBVgcYx52tpZh+Y2YagxKRNzL7qZvasma0I\n7vuVYPuBZvaama0Otr9mZvXKEAdmVtHMNuP/hmaZ2YJg+zHBa70huPcuMc8ZaWZPmNkbwXPzSrmG\nmdkfg5K2/5rZc7FJmJn1CvatCY6LfV8MNLMxwXIlMxtjZmuDuP5tZgeb2VDgdOCx4LX6S3D8LjM7\nPFjez8weDK6zwczeM7NKRcTaJngP9zOzVcCzwfbOweuyIXjvnxDznOZm9p/gvfCimY23oJpuL8/X\n38yWB/fylZmdGWxP+d+IiJSBc06PBD2AxUDbYLk+8AUwKFifBiwBjsZ/eOUAfwOewH8Y1wQ+AnoH\nx18ILAOaB+uHA/WLuM6HQM9guQrQIlhuCOwEsoL194BHgQpAM2A1kBfsGwhsAdrjE6lhwPQS7rMH\ncGBwH78DVgEVSztXcO0lwG+BbOB84CfgrmKu8xvgvWA5G+gD/AjUDLbVBdYC7YP1dsF6jWD9DWAc\nUC14/unB9oOAbkAlfGIzAfhbzHWnAVcVjqGYGHcBhwXLOcACoH+wfCawCWgc7B8JbABaBusVizhf\n7LWvAuYHr2UVYCIwOth3LLAZaBVc635gW8z7YmDMsdcCk4L7NeBXwP6FrxcTw07g8GD5ceCfwCHB\nc1sCFYqIuw2wPXi9KwTX+hXwLb5k0IDL8e/dCjHvhZuC16ZbEP9de3m+o4BvgNrB8xvEvC4p/xvR\nQw894n+EHkAmPYJ/ipuA9cHyo0ClYN80gqQkWK+F/1CtFLPtEmBqsPw2cHMJ19n9gZMf/JOsUeiY\ngn+y+KRoO1AlZv8w4NlgeSAwJWbfMcAPZbjv9cAJpZ0LOANYXui5H1ByIrI9OP9PwA/ABTH7+wGj\nCj3n7eAD6pDg/qvFEf+JwLqY9bImIrs/tE8DVhbaPxa4M1geCTxXSiyx1/4HcH3MvqPwH9ZZwJ+A\nF2L2Vab4RORK4F+7X6Pirlf4noIP3C3A8XH8DtsE7+cKMdueAAYXOm4uvhTmdGBZoX3vs2ciUpbz\nHQH8F5+M5hQ6Jp+Q/0b00EOP4h+qmkm8rs65g5xzhznnbnbObYvZtyxmuSH+m9cqM1tvZhuAp4CD\ng/31ga/juN7VQBNgblDkXlRjzjrAeufclphtS4HY6oj/xixvAfazYtowmNlt5quMNgRxV8OX6JR2\nrjrAikKnW1rCvYH/1nkQvgRmMj6Z2a0hcFHw+9v9O2wdXKc+PrnYVET8lc3sr0F1w3fAu8CBZvvc\nbqcue77G8Mvfc+H9pZ0v9vezFF/6UbvwtZxzW4F1xZxnDPB3fPua5WZ2n8XX5qImviRiUZzxrnHO\nbY9Zbwj8vtDrc2gQe11++V4o/LuJ+3zOua+BvsAg4FszG2tmdYLnpfxvRETipz+ixCvpw8zFLC/D\nf+OrESQu1Z1zBzrnmsbsP6K0iznnvnbO9XDOHQz8GXjZzCoXOmwlcJCZVY3Z1oBffhCUynx7kP/F\nl0xUd85Vx5cCxfMhvoo9/7HvjqNUwQfEjcDl9nM32WX4b/0HxfwOc51zfw72HWRFN2z9Pb6dySnO\nN4LdndzsayKyEp8AxSr8e3bEbyX+w3e3hsAOfPXEKvyHMOCTK6BGUSdxzu1wzg1xzh0HnAp0BnrF\nEc9a/Hu01PdhMedaBtxd6PXZ3zk3gaLfC4V/d2U5H8658c650/n5d3ZvsD2lfyMiUjZKRELinPsv\nMAX4PzPLDRomHm4/jw/yDHDb7gZxZnaEmRX+R42Z9TSz3aURG/H/vHft3h1cazm+nvyeoOFiU/y3\nxDElhFjch3Iuvgh7nfkGm3cG20qy+1zTgR1mdrOZ5ZhZd6BFKc8t4JzbAAzHF5MDPA90MbOzzSwr\naFjZxszqBr/ft4AnzDdOrWBmp8fcw1Zgk5kdhP8WnQj/BrYEDSxzzCwP/6E/bi/PNw74nZk1MrP9\ngbuB8c65XcDL+HtvaWYVKOEezCzPzI4Pvr1/j3/9dga7v8VXw/yCc87hq5MeChpyZsVcLx7DgevN\nrEUQR1Uz6xh82E8HdppZHzPLNrOulP5eKPZ8ZnaUmZ1pZhXx1XhbCf4OQvgbEZEyUCKSWCV9uyxq\nXy+gIr7753rgJXzbBpxzL+M/eMaa2SZ8w9aDijhXB2BOcMz/ARfHVAfFHncpcBj+m99E4E/OuWl7\ncS9/Dx7z8W1VtlB6dYML7mk70B3fZmEdvkHuxFKeW9gjwDlmdnzw4dEV+AOwBl+Ufhs/v68vx5cg\nzMUXq98SbH8Y32hxLf7D582i4o1TwbHB/XUBOgbnfgy43Dm3oAznjT3mWfwH4Xv4arot+Ia+OOe+\nBG7GN7RdiS+VWo1vJ1LYIfjEZSMwB98u5Plg3yPAhUFPkYeLiOE2YDYwA/+a3Uuc/zecc58CvfG9\nctbj3zO/Cfbtfi9cg2/A2wN4rZj4Sz0fvgrpXvz7YCW+ivP2YF+q/0ZEpAzMf+kRkXQWlDJ8Bxzp\nnCut3U0kmdlHwJPOuVFhxyIiqaMSEZE0ZX5MjcpBEvIgMCudkhAzO8PMagdVM78BTsD3ehKRckSJ\niEj66oqvRliOb1B6SbjhlFkTYCa+auZ3wPnOuW/DDUlEUk1VMyIiIhIalYiIiIhIaHLCDiBeZqai\nGxGRveCc26euxpUrV/7vjz/+WDtR8Uj5s99++327devWQ4ral1YlIns7fOzAgQNDH8I21Q/dc/l4\n6J7Lx2Nf7jkRfvzxx9ph/w70SO9HSYlsWiUiIiIiklmUiIiIiEhoykUikpeXF3YIKad7Lh90z+VD\nebxnKT+S2n3XzEbg59r41v08mVvhY/4CnIOf4v0K59znxRznkhmriEgmMjPcPjZWTef/v4MHD2bh\nwoWMGVPStEGZLSsri4ULF3L44UVOK1Xg3Xff5bLLLmPZsrJMEh6fkt6HyS4RGQm0L26nmZ0DHOGc\nawxcBzyV5HhERCTDjB07llNOOYXc3Fzq1atHp06d+PDDDwv2m6VufsI777yTpk2bUqFCBe66664y\nPTcvL4+srCxmz569x/Zu3bqRlZXFe++9t1cxleX+y3Ls0qVLadu2LVWrVuXYY49l6tSpexNechMR\n59y/8KMmFqcrMDo49t/AAWamLmIiIhKXhx56iFtvvZU//vGPrF69mm+++YY+ffowefLkUOJp3Lgx\n999/P507dy7zc82MJk2aMHr06IJt69ev56OPPqJWrVp7HVOySrMuvfRSTjrpJNavX8/QoUO54IIL\nWLduXZnPE3YbkXrsOXPrimCbiIhIiTZt2sTAgQN54okn6Nq1K5UrVyY7O5uOHTty7733Fvmciy66\niDp16lC9enXy8vL48ssvC/a9+eabHHfccVSrVo369evz0EMPAbBu3Tq6dOlC9erVqVGjBm3atCk2\npssvv5z27duz//7779U99ezZkwkTJhQkD+PGjaN79+5UrFix4JiffvqJvn37Uq9ePQ499FB+97vf\nsX379oL9999/P3Xr1uXQQw9l5MiRe5Ry/PTTT9x22200bNiQOnXqcOONN7JtW7GTXhdrwYIFfPbZ\nZwwaNIhKlSrRvXt3mjZtysSJZZ1QPfxEJOlWr4aOHWHVqrAjERGRRJo+fTrbtm3jvPPOi/s5HTt2\n5Ouvv2b16tU0b96cnj17Fuy75pprGD58OJs2beKLL76gbdu2ADz44IPUr1+fdevWsXr1aoYNG5bw\ne9mtbt26HHvssUyZMgWA0aNH06tXrz1KNYYOHcrHH3/MrFmzmDlzJh9//DFDhw4F4O233+ahhx5i\n6tSpLFiwgH/84x97nL9///4sXLiQWbNmsXDhQlasWFFsFVKfPn246aabitw3Z84cDj/8cKpWrVqw\nrVmzZsyZM6fM9xz2yKorgPox64cG24o0aNCgguW8vLy4WpIffDC0bg2tWsEbb8Bxx+11rCIikZef\nn09+fn5Kr5moJhhlrUFYt24dNWvWJCsr/u/UV1xxRcHynXfeycMPP8zmzZvJzc2lYsWKzJkzhxNO\nOIEDDjiAE088EYAKFSqwatUqFi9ezBFHHEHr1q3LFmgZ9erVi1GjRtGoUSM2btzIr3/96z32jx07\nlscff5waNWoAMHDgQK6//noGDx7MSy+9xJVXXskxxxwD+M/NcePGFTx3+PDhzJ49mwMOOACAAQMG\n0LNnT+6+++5fxPH4448XG+P3339fcI7dqlWrxsqVK8t8v6lIRCx4FGUy0AeYYGYtge9cCbNvxiYi\ncV/c4I47oFEjOPNMGDcO2rUr82lERNJC4S9pgwcPTvo1w+pQU6NGDdauXcuuXbviSkZ27drFH/7w\nB15++WXWrl2LmWFmrF27ltzcXCZOnMiQIUPo378/zZo145577qFly5b069ePgQMHcvbZZ2Nm9O7d\nm/79+yftvrp168att95KjRo1uPzyy3+xf+XKlTRo0KBgvWHDhgUJwMqVKzn55JP32LfbmjVr2LJl\nCyeddFLBtl27du1VG5L999+fTZs27bFt48aN5ObmlvlcSa2aMbOxwIfAUWb2jZldaWbXmdm1AM65\nN4HFZrYQ+CtwY7Ji6dkTXnoJevSAUaOSdRUREUmVVq1aUalSJV599dW4jn/hhRd47bXX+Oc//8l3\n333HkiVLCoYgBzjppJN49dVXWbNmDV27duWiiy4CoGrVqjzwwAN8/fXXTJ48mYceeohp06Yl7b4q\nV67MOeecw1NPPUWvXr1+sb9u3bosXbq0YH3p0qXUrVsXgDp16uzR/Xbp0qUFbURq1qxJlSpVmDNn\nDuvXr2f9+vV89913bNy4scwxHnfccSxatIgffvihYNvMmTM5bi+qHZLda6aHc66uc66Sc66Bc26k\nc+6vzrmnY465yTl3pHOumXPuP8mMp00byM+HwYNh0KDwsngREdl31apVY/DgwfTp04dJkyaxdetW\nduzYwVtvvcWAAQN+cfz3339PpUqVqF69Oj/88AO33357wYf09u3bGTt2LJs2bSI7O5vc3Fyys7MB\neOONN/j6668ByM3NJScnp9gSmB07dvDjjz+ya9cutm/fzrZt29i1axfgk4KsrCy++eabUu/tnnvu\n4d1336V+/fq/2HfppZcydOhQ1q5dy9q1axkyZEhByclFF13Ec889x1dffcWWLVv2aP+xuzSnb9++\nrFmzBoAVK1YUtEcpi8aNG3PiiScyePBgtm3bxiuvvMIXX3zB+eefX+ZzZXxj1cKOOQamT4c334Qr\nroCffgo7IhER2Vu33norDz30EEOHDqVWrVo0aNCAJ554osgGrL169aJBgwbUq1eP448/nlNPPXWP\n/WPGjOGwww7jwAMP5Omnn2bs2LGA7yFy1llnkZubS+vWrenTp0+xPWd69+5NlSpVGD9+PMOGDaNK\nlSo8//zzAHzzzTc0atSIevWK7hwa27vlkEMO2SO+2H1//OMfOfnkk2natCnNmjXj5JNP5o477gCg\nQ4cO9O3bl7Zt23LUUUfRrlBbhPvuu48jjzySli1bcuCBB3L22Wczf/78IuO54YYbuPHG4isqxo8f\nz4wZM6hevTp33HEHEydOLGi3UhZJHVk1kRI9st8PP/jqms2bYeJEOPDAhJ1aRCQyyvvIqlFy9913\nU6tWLXr37h12KClX0vuw3CYiADt3wq23wtSpvkdNTJseEZGMoEREoiDMId4jLTsbHnkEeveGU0+F\nTz8NOyIREZHypVwnIrvdcgs89hh06ACvvx52NCIiIuWHEpFAt24+CendG554IuxoREREyody3Uak\nKIsW+SHhu3SB++6DMgzYJyISOWojIlGgxqpltH49nHce1K4No0dD5copuayISMIpEZEoUCKyF7Zt\ngyuvhCVLYNIkP2eNiEi6SUQiUrly5f/++OOPtRMVk5Q/++2337dbt249pKh9SkRKsGsX/OlP8OKL\nfgC0xo1TenkRkX2WiEREJJnCnn030rKy4O67/YR5p5/uBz5L8qSLIiIi5YqaYsahd2947jnfs+al\nl8KORkREJHOoaqYMZs6Ezp3ht7+F224DU2GniEScqmYk6pSIlNHy5dCpkx+J9dFHIUeVWyISYUpE\nJOqUiOyFTZvgwgt9EjJhAuy/f9gRiYgUTYmIRJ3aiOyFatX8KKx16sAZZ8DKlWFHJCIikp6UiOyl\nChVg+HC44AJfTfPFF2FHJCIikn5UNZMAY8dC374wbhy0axd2NCIiP1PVjESdSkQSoEcP3623Rw8Y\nNSrsaERERNKHSkQSaO5cP2Fer14wcKC694pI+FQiIlGnRCTBvv3Wz9x7zDG+DUnFimFHJCLlmRIR\niTpVzSRY7dqQnw8bN0KHDvDdd2FHJCIiEl1KRJKgShU/L80JJ/i5aZYuDTsiERGRaFIikiTZ2fDI\nI3Dttb577yefhB2RiIhI9KiNSAq8+qqfOG/kSD9XjYhIqqiNiESdSkRS4Lzz/Eis114LTzwRdjQi\nIiLRoRKRFFq0yHfv7dwZ/vxnyFIaKCJJphIRiTolIim2fj106wa1asHo0VC5ctgRiUgmUyIiUafv\n5Cl20EEwZYofX6RdO1izJuyIREREwqNEJASVKsGYMXDmmb5HzYIFYUckIiISjpywAyivsrLg7rvh\nsMPg9NP9uCOtW4cdlYiISGqpRCRk11zjJ8rr1g1efDHsaERERFIr6YmImXUws7lmNt/M+hexv5qZ\nTTazz81stpldkeyYoqZ9e3jnHfj9731vmgxokysiIhKXpPaaMbMsYD7QDlgJzAAucc7NjTnmdqCa\nc+52M6sJzANqO+d2FDpXRvSaKcny5dCpk2838uijkKOKMxHZR+o1I1GX7BKRFsAC59xS59x2YDzQ\ntdAxDsgNlnOBdYWTkPLi0EPh/fdh8WLo2hW+/z7siERERJIr2YlIPWBZzPryYFusx4BjzWwlMBO4\nJckxRVq1avDaa1C3LpxxBqxcGXZEIiIiyROFwv/2wGfOubZmdgTwjpk1dc79ojxg0KBBBct5eXnk\n5eWlLMhUqlABnn4a7rkHWrWCN96A448POyoRSQf5+fnk5+eHHYZI3JLdRqQlMMg51yFYHwA459x9\nMce8DtzjnPsgWJ8K9HfOfVLoXBnfRqQoY8dC377+51lnhR2NiKQbtRGRqEt21cwM4Egza2hmFYFL\ngMmFjlkKnAVgZrWBo4BFSY4rbfToAS+/DD17wnPPhR2NiIhIYiV9rhkz6wA8gk96Rjjn7jWz6/Al\nI0+bWR3gOaBO8JR7nHPjijhPuSwR2W3uXD9h3uWXw6BBYPp+IyJxUImIRJ0mvUsj334L554LTZrA\nM8/4+WpEREqiRESiTiOrppHatWHaNNi8GTp0gA0bwo5IRERk3ygRSTNVqvg2I02b+rlpliwJOyIR\nEZG9p0QkDWVnw8MPw3XX+WTkk09Kf46IiEgUqY1Imnv1Vbj2WhgxArp0CTsaEYkatRGRqIvCgGay\nD847z4/Cet558M030KdP2BGJiIjETyUiGWLRIt+9t3NnP4NvlirdRASViEj0KRHJIOvXQ7ducPDB\nMGYMVK4cdkQiEjYlIhJ1+t6cQQ46CKZMgUqVoG1bWLMm7IhERERKpkQkw1SqBM8/D+3a+Qnz5s8P\nOyIREZHiqbFqBjKDoUOhUSM44wyYONF38xUREYkalYhksGuugdGjfbuRCRPCjkZEROSX1Fi1HJg5\n048x0qcP9OunCfNEyhM1VpWoUyJSTqxYAZ06+XYjjz4KOaqUEykXlIhI1CkRKUc2bYKLLvJjjEyY\nALm5YUckIsmmRESiTm1EypFq1eC116BePWjTBlauDDsiEREp75SIlDMVKsDTT8OFF/pqmtmzw45I\nRETKM1XNlGPjxsEtt8DYsXDWWWFHIyLJoKoZiTqViJRjl14KL78MPXvCyJFhRyMiIuWRSkSEefP8\nhHk9e8LgwereK5JJVCIiUadERABYvdqPNdKkCTzzDFSsGHZEIpIISkQk6lQ1IwDUqgXTpsHmzdC+\nPWzYEHZEIiJSHigRkQJVqvg2Iyee6OemWbIk7IhERCTTKRGRPWRnw//9H1x/vU9GPvkk7IhERCST\nqY2IFGvSJOjdG0aM8O1HRCT9qI2IRJ1mHJFide0KderAeefB0qVw001hRyQiIplGJSJSqsWLfffe\njh3h/vv9XDUikh5UIiJRp0RE4rJhA3TrBjVrwpgxULly2BGJSDyUiEjU6butxKV6dfj732G//aBt\nW1izJuyIREQkEygRkbhVquRLQ846y0+YN39+2BGJiEi6U2NVKRMzGDIEGjWCM87w446cdlrYUYmI\nSLpSiYjslauvhtGjoXt3mDAh7GhERCRdqbGq7JNZs6BzZ+jTB/r104R5IlGjxqoSdUkvETGzDmY2\n18zmm1n/Yo7JM7PPzOwLM5uW7JgkcZo2henTYdw4uOEG2LEj7IhERCSdJLVExMyygPlAO2AlMAO4\nxDk3N+aYA4APgbOdcyvMrKZzbm0R51KJSIRt3gwXXeSXX3wRcnPDjUdEPJWISNQlu0SkBbDAObfU\nObcdGA90LXRMD2Cic24FQFFJiERfbi5Mngz16/tGrCtXhh2RiIikg2QnIvWAZTHry4NtsY4CDjKz\naWY2w8wuT3JMkiQVKsBf/woXX+y7986eHXZEIiISdXF33zWzekDD2Oc4595LUAzNgbZAVWC6mU13\nzi0sfOCgQYMKlvPy8sjLy0vA5SWRzGDAAN+9t107eOEF+J//CTsqkfIjPz+f/Pz8sMMQiVtcbUTM\n7D7gYuBLYGew2Tnnzi3leS2BQc65DsH6gOB598Uc0x/Yzzk3OFh/BnjLOTex0LnURiTNvP8+XHgh\nDBsGV10VdjQi5ZPaiEjUxZuIzAOaOue2lenkZtnAPHxj1VXAx8ClzrmvYo45GngU6ABUAv4NXOyc\n+7LQuZSIpKF58/xkeT16wF13qXuvSKopEZGoi7eNyCKgQllP7pzbCdwETAHmAOOdc1+Z2XVmdm1w\nzFzg78As4CPg6cJJiKSvJk1899533oFevWBbmVJZERHJdPGWiEwEmgFTgYKPEufcb5MX2i9iUIlI\nGtuyBS67zM/i+8orfhI9EUk+lYhI1MWbiPymqO3OuVEJj6j4GJSIpLmdO+F//xfefhvefNM3aBWR\n5FIiIlEX94BmZlYR39UWYF4wLkjKKBHJHI8+CvfeC5Mmwcknhx2NSGZTIiJRF2+JSB4wClgCGFAf\n+E2Cuu/GRYlIZpk0CXr3hmeegXNL7HslIvtCiYhEXbzjiDyIH4J9HoCZHQWMA05KVmCS2bp2hbp1\n/c9vvoGbbgo7IhERCUO8vWYq7E5CAJxz89mLXjQisU45BT74AB5/HG69FXbtCjsiERFJtXirZp4F\ndgHPB5t6AtnOuZQNU6Wqmcy1YQN06wY1asCYMVClStgRiWQOVc1I1MWbiFQC+gCnBZveB54o6wBn\n+0KJSGbbtg2uvhoWLvST59WqFXZEIplBiYhEXdy9ZsKmRCTzOQd33gljx/ruvU2ahB2RSPpTIiJR\nV2JjVTN70Tl3kZnNBn6RBTjnmiYtMil3zGDIEDjsMGjTBl56CU4/PeyoREQkmUosETGzOs65VWbW\nsKj9zrmlSYvsl7GoRKQcmTLFj8T66KNw8cVhRyOSvlQiIlEXbxuRqsBW59yuoOvu0fgZclM2qJkS\nkfJn9mzo1An69IF+/TRhnsjeUCIiURdvIvIpcDpQHfgAmAH85Jzrmdzw9ohBiUg5tGIFdO4MLVr4\nbr458Y58IyKAEhGJvnjHETHn3BagO763zIXAcckLS8SrVw/ee88PetalC2zeHHZEIiKSSHEnImbW\nCj9+yBvBtuzkhCSyp9xceO01aNAAzjjDl5KIiEhmiDcR6QvcDvzNOTfHzA4HpiUvLJE95eTAU0/B\nJZdAq1Ywa1bYEYmISCJoHBFJO+PHw29/C88/D2efHXY0ItGmNiISdaV1333YOdfXzF6j6HFEUjZv\nqhIRifX++3DhhTBsGFyVsokGRNKPEhGJutISkZOcc5+aWZui9jvn3k1aZL+MRYmI7GHePOjYEXr0\ngLvuUvdekaIoEZGoK/M4IsF6NlAp6EmTEkpEpCirV8O558KRR8KIEVCpUtgRiUSLEhGJungbq04F\nYudErQz8I/HhiJRNrVowbRps3QodOviZfEVEJH3Em4js55z7fvdKsKzJ2iUSKleGF1+E5s3h1FNh\n8eKwIxIRkXjFm4j8YGbNd6+Y2UnA1uSEJFJ22dnw4IN+OPjWrWHGjLAjEhGReMTbRuQUYDywEjDg\nEOBi59wARKMAAAAT3klEQVSnyQ1vjxjURkTiMnkyXH01PPMMdO0adjQi4VIbEYm6uMcRMbMKQJNg\ndV4qJ7wLrq9EROI2Ywacdx4MGAA33xx2NCLhUSIiURdviUgV4FagoXOut5k1Bpo4515PdoAxMSgR\nkTJZssR3723fHh54wFffiJQ3SkQk6uJtIzIS+AloFayvAIYmJSKRBGnUCD74AD7/3A9+tiVlnc1F\nRCRe8SYiRzjn/gxsBwjGD1GGLZFXvTq8/TZUrQpt2/pxR0REJDriTUR+MrPKBMO8m9kRwLakRSWS\nQJUqwejRfl6aVq38iKwiIhINOXEeNxB4G6hvZi8ArYErkhWUSKKZ+WHgGzWCM86Al1+G008POyoR\nESm1saqZGXAosAVoia+S+cg5tzb54e0RhxqrSkK88w707Al/+QtccknY0YgklxqrStTF22tmtnPu\nhBTEU1IMSkQkYWbPhs6d4YYboH9/TZgnmUuJiERdvG1E/hMMalZmZtbBzOaa2Xwz61/CcaeY2XYz\n67431xEpixNOgOnTYcIEuO462LEj7IhERMqneEtE5gKNgSXAD/jqGeeca1rK87KA+UA7/KisM4BL\nnHNzizjuHfyw8c86514p4lwqEZGE27wZLr4YnPPz1eTmhh2RSGKpRESiLt4SkfbA4UBboAvQOfhZ\nmhbAAufc0mAk1vFAUYNu3wy8DKhzpaRUbq4fEr5hQ994dcWKsCMSESlfSkxEzGw/M+sL/C/QAVgR\nJBVLnXNL4zh/PWBZzPryYFvsNeoC5znnnkRjk0gIcnLgySehRw/fvXfWrLAjEhEpP0rrvjsKP4jZ\n+8A5wLHALQmO4WEgtu1IscnIoEGDCpbz8vLIy8tLcChSXplBv36+ZOSss+D55/24IyLpJj8/n/z8\n/LDDEIlbiW1EYnvLmFkO8LFzrnncJzdrCQxyznUI1gfg25bcF3PMot2LQE18G5RrnXOTC51LbUQk\nJf71L7jgAhg2DK66KuxoRPaN2ohI1JVWIlIww65zboeVvY/jDOBIM2sIrAIuAS6NPcA5d/juZTMb\nCbxWOAkRSaXTToP33vMT5i1aBEOGqHuviEiylJaINDOzTcGyAZWD9d29ZqqV9GTn3E4zuwmYgm+P\nMsI595WZXRc8/+nCTyn7LYgk3lFH+e69554LixfDs8/6oeJFRCSx4uq+GwWqmpEwbN0Kl10G69bB\n3/7mJ9ETSSeqmpGoi7f7rki5VLkyvPQSnHQSnHqqLx0REZHEUSIiUoqsLHjwQejTB1q3hhkzwo5I\nRCRzqGpGpAxeew2uvhqGD4euRQ3NJxIxqpqRqFOJiEgZdOkCb74JN97oZ+8VEZF9oxIRkb2wZInv\n3tu+PTzwAGRnhx2RSNFUIiJRp0REZC9t2ADdu/ueNM8/D1WqhB2RyC8pEZGoU9WMyF6qXh3+/nfY\nf38480xYrSkbRUTKTImIyD6oWBFGjfJVNK1awbx5YUckIpJeShtZVURKYQZ33QWHHQZt2vhxR04/\nPeyoRETSg0pERBLkyit9W5Hzz4dx48KORkQkPaixqkiCzZ4NnTvD9dfDgAGaME/CpcaqEnVKRESS\nYOVK6NQJTjkFHn8cKlQIOyIpr5SISNSpakYkCerWhffegxUr/CBomzeHHZGISDQpERFJktxcmDQJ\nGjXyjVeXLw87IhGR6FEiIpJEOTnw5JPQo4efvXfWrLAjEhGJFrUREUmRCRPg5pt9z5qzzw47Gikv\n1EZEok4lIiIpcvHF8Mor0KsXjBgRdjQiItGgEhGRFJs/30+Yd8klMGSIuvdKcqlERKJOiYhICNas\ngXPPhcMPh2efhUqVwo5IMpUSEYk6Vc2IhODgg+Gf/4Rt23x7kfXrw45IRCQcSkREQlK5Mrz4oh/0\nrHVrWLw47IhERFJPiYhIiLKy4IEH4KabfDLy8cdhRyQiklpKREQioE8f+Otf/Rw1r74adjQiIqmj\nxqoiEfLJJ9C1K/TrB7fcEnY0kgnUWFWiTomISMQsWeK793bsCPffr+69sm+UiEjUKRERiaANG3w1\nTVaWHwitdWs48UQlJVJ2SkQk6pSIiETUjh3wt7/BlCmQnw+1asHVV8OvfgXNmvkkRaQ0SkQk6pSI\niKQB53xj1g8/hOnTYcUKaNcO7r0Xjjsu7OgkypSISNQpERFJQ2vXwvDhMHgwtGkDJ58Md90F2dlh\nRyZRo0REok6FuyJpqGZNuP12XzJy/PEwbBjk5MBpp/nqHBGRdKESEZEMsH07LF3q25P06eNn+B04\n0M9lI+WbSkQk6lQiIpIBKlSAI4+EG2+EOXN875pf/9q3JxERibKkJyJm1sHM5prZfDPrX8T+HmY2\nM3j8y8xOSHZMIpns2GPhuefgttvg1FPhnHPg66/DjkpEpGhJTUTMLAt4DGgPHAdcamZHFzpsEXCG\nc64ZMBQYnsyYRMqL/v1h9mw/QNqRR/oB0saOhS1bwo5MRORnyS4RaQEscM4tdc5tB8YDXWMPcM59\n5JzbGKx+BNRLckwi5cbxx8NXX/luv3XqQM+eULUqnHUW3HwzfPdd2BGKSHmX7ESkHrAsZn05JSca\n1wBvJTUikXKoVSsYMcKPR/L++9C4MTzxBFSv7hOULl3gtdfCjlJEyqOcsAPYzczOBK4ETivumEGD\nBhUs5+XlkZeXl/S4RDLNaaf5x5NPwg8/wNSp8NZbcO65fv8550CjRr57cP36oYYqeyE/P5/8/Pyw\nwxCJW1K775pZS2CQc65DsD4AcM65+wod1xSYCHRwzhXZrE7dd0WSa/NmePttWLwYJk6Ejz+Go4/2\n45IcXbhll6QNdd+VqEt2IpINzAPaAauAj4FLnXNfxRzTAJgKXO6c+6iEcykREUmhZcvgjjtgzBgY\nMsQva9K99KNERKIu6QOamVkH4BF8e5QRzrl7zew6fMnI02Y2HOgOLAUM2O6ca1HEeZSIiITgvff8\nMPIAL7wAPXqEG4+UjRIRiTqNrCoipdqxA8aPh8svhxYtfDLSti00aQIVK4YdnZREiYhEnUZWFZFS\n5eTAZZfBypVw5pnwyCPQtClUqgQ1asANN8CGDWFHKSLpSImIiMStTh24915YtMh3BV61Ctq3h/x8\nOOgg34bk3HPh88/DjlRE0oUSERHZa4cc4kdr/eorP/He66/7AdN+9Sto1sxPvLdwYdhRikiUKRER\nkYTIyYFOnWDcOD+3TYsWfpC0xo3hhBP8WCUiIoWpsaqIJNV//uMHT3vmGTjlFPjgAz9bsKSGGqtK\n1KlERESSqnlzGD4cPvsM1q71vWy6dPGjuoqIqERERFLGOT8jcLNmfv33v4e+feHQQ8ONK5OpRESi\nTiUiIpIyZr7b77Zt8NRTvtqmfn3fwPXZZ+H778OOUERSTSUiIhKqHTv8TMCPPQYLFviB0k4+2Q8r\nr8HS9p1KRCTqlIiISCQ450tIXn0Vhg7127p2he7d4fzzfamJlJ0SEYk6JSIiEjk7d/quv2+95ccp\n2V1l86c/+dKSzp0hSxXLcVEiIlGnREREIm/zZl9988EHMHWqb2ty881w++1QrVrY0UWbEhGJOiUi\nIpJWduyAKVPg4YfhnXfg7LPhxRfhgAPCjiyalIhI1KlwU0TSSk4OdOzok5GlS301zoEHauRWkXSl\nRERE0laDBvCPf/i2Ix07wmGHwahRYUclImWhRERE0t5dd8G6dXDVVXDFFb6a5g9/8HPeiEi0qY2I\niGSUzZt9qcjIkb47cKtW0KGDn3ivaVM44oiwI0wttRGRqFMiIiIZa9UqeOEF+PxzmDXLDy8PPiG5\n6CJo185PxJedHW6cyaRERKJOiYiIlCtr1vheNpMnw6ef+iqdTp1g2DCfoGQaJSISdUpERKRce/dd\n357kww/hxBNh3Dg4+uiwo0ocJSISdWqsKiLlWps2fqC0DRt8O5JjjvHz3uh7j0hqqERERCTGn/8M\nAwb4tiNvvAE1a4Yd0b5RiYhEnUpERERi9OsH334LtWrBwQfDnDlhRySS2ZSIiIgUcvDBftK9G26A\n44/389rsnnhPRBJLVTMiIiWYOhW6dIGtW6F2bd+gtUULOPZY/2jcGCpXDjvK4qlqRqJOiYiISBz+\n+18/HskXX/jHnDmwaBGsX+8TkeOO8+1K2rSB006DevXCjthTIiJRp0RERGQfbNzoJ9+bNw8++QSm\nT4f334fmzeH5530vnDApEZGoUyIiIpJgGzbATTfB2LG+jcngwdC9ezixKBGRqFNjVRGRBKte3Q8t\n/9NPcP31cP75mhVYpDgqERERSbI33oDOnWHaNMjLS+21VSIiUacSERGRJOvUCW67Dc48ExYvDjsa\nkWhJeiJiZh3MbK6ZzTez/sUc8xczW2Bmn5vZicmOSUQk1e6/H7p1871rliwJOxqR6EhqImJmWcBj\nQHvgOOBSMzu60DHnAEc45xoD1wFPJTMmEZGwjBzp24scdhjcey/s3Bl2RCLhS3aJSAtggXNuqXNu\nOzAe6FromK7AaADn3L+BA8ysdpLjEhFJuQMOgDFjfG+ahx6CnBzo2ROGD4eZM2H79rAjFEm9ZCci\n9YBlMevLg20lHbOiiGNERDLGpZf6+WxeegmaNIFJk3z7kYoV/SR7nTrB3XfD66/DypUqOZHMlhN2\nACIi5ZEZXHDBntu2bPGjt86a5QdHmzwZZs+GV1+Fs88OJ06RZEt2IrICaBCzfmiwrfAx9Us5BoBB\ngwYVLOfl5ZGX6n5wIiJJVKUKnHqqf+yt/Px88vPzExaTSLIldRwRM8sG5gHtgFXAx8ClzrmvYo7p\nCPRxznUys5bAw865lkWcS+OIiIiUkcYRkahLaomIc26nmd0ETMG3RxnhnPvKzK7zu93Tzrk3zayj\nmS0EfgCuTGZMIiIiEh0aWVVEJIOpRESiTiOrioiISGiUiIiIiEhoykUiUh5bkOueywfdc/lQHu9Z\nyg8lIhlK91w+6J7Lh/J4z1J+lItERERERKJJiYiIiIiEJq2674Ydg4hIOlL3XYmytElEREREJPOo\nakZERERCo0REREREQpNRiYiZdTCzuWY238z6F3PMX8xsgZl9bmYnpjrGRCvtns2sh5nNDB7/MrMT\nwogzkeJ5nYPjTjGz7WbWPZXxJUOc7+08M/vMzL4ws2mpjjHR4nhvVzOzycHf8mwzuyKEMBPGzEaY\n2bdmNquEYzLq/5cIAM65jHjgk6qFQEOgAvA5cHShY84B3giWfw18FHbcKbjnlsABwXKH8nDPMcdN\nBV4Huocddwpe5wOAOUC9YL1m2HGn4J5vB+7Zfb/AOiAn7Nj34Z5PA04EZhWzP6P+f+mhx+5HJpWI\ntAAWOOeWOue2A+OBroWO6QqMBnDO/Rs4wMxqpzbMhCr1np1zHznnNgarHwH1UhxjosXzOgPcDLwM\nrE5lcEkSzz33ACY651YAOOfWpjjGRIvnnh2QGyznAuuccztSGGNCOef+BWwo4ZBM+/8lAmRW1Uw9\nYFnM+nJ++aFb+JgVRRyTTuK551jXAG8lNaLkK/WezawucJ5z7kkgE7otxvM6HwUcZGbTzGyGmV2e\nsuiSI557fgw41sxWAjOBW1IUW1gy7f+XCAA5YQcgqWFmZwJX4ot/M93DQGybgkxIRkqTAzQH2gJV\ngelmNt05tzDcsJKqPfCZc66tmR0BvGNmTZ1z34cdmIjEL5MSkRVAg5j1Q4NthY+pX8ox6SSee8bM\nmgJPAx2ccyUV/aaDeO75ZGC8mRm+7cA5ZrbdOTc5RTEmWjz3vBxY65z7EfjRzN4DmuHbWaSjeO75\nSuAeAOfc12a2GDga+CQlEaZepv3/EgEyq2pmBnCkmTU0s4rAJUDhD57JQC8AM2sJfOec+za1YSZU\nqfdsZg2AicDlzrmvQ4gx0Uq9Z+fc4cHjMHw7kRvTOAmB+N7bk4DTzCzbzKrgGzN+leI4Eymee14K\nnAUQtJU4CliU0igTzyi+BC/T/n+JABlUIuKc22lmNwFT8AnWCOfcV2Z2nd/tnnbOvWlmHc1sIfAD\n/htV2ornnoE/AQcBTwQlBNudcy3Ci3rfxHnPezwl5UEmWJzv7blm9ndgFrATeNo592WIYe+TOF/n\nocBzMd1d+znn1ocU8j4zs7FAHlDDzL4BBgIVydD/XyK7aYh3ERERCU0mVc2IiIhImlEiIiIiIqFR\nIiIiIiKhUSIiIiIioVEiIiIiIqFRIiIiIiKhUSIiEjCznWb2n2BK+UlmVi3B5/+Nmf0lWB5oZrcm\n8vwiIulIiYjIz35wzjV3zp2AnwW1T9gBiYhkOiUiIkWbTszMpmZ2m5l9bGafm9nAmO29zGymmX1m\nZqOCbZ3N7CMz+9TMppjZwSHELyKSFjJmiHeRBDAAM8sG2gHPBOv/AzR2zrUIhsmfbGanAeuBPwCt\nnHMbzOzA4DzvO+daBs+9Gj8T8G2pvRURkfSgRETkZ5XN7D/4WU2/BN4Jtp8N/E+wz4CqQOPg50u7\nZzR2zn0XHF/fzF4E6gAVgMWpuwURkfSiqhmRn21xzjXHTz9v/NxGxIB7gvYjv3LOHeWcG1nCeR4F\n/uKcawpcD+yX1KhFRNKYEhGRnxmAc+5H4BbgNjPLAv4OXGVmVQHMrG7Q7uOfwIVmdlCwvXpwnmrA\nymD5NymMX0Qk7ahqRuRnBVNRO+c+N7OZwKXOuRfM7Bhgum8iwmbgMufcl2Z2N/Cume0APgOuAgYD\nL5vZenyy0ijF9yEikjbMOVf6USIiIiJJoKoZERERCY0SEREREQmNEhEREREJjRIRERERCY0SERER\nEQmNEhEREREJjRIRERERCY0SEREREQnN/wOnlCydnB8fwAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3701dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# n-bagging of estimators\n",
    "n = 1\n",
    "models = [LogisticRegression(C=1e5) for _ in xrange(n)]\n",
    "D_plus = G_train.edges()\n",
    "mean_avg_precision = 0.0\n",
    "for i,model in enumerate(models):\n",
    "    D_minus = valid_random_non_edges(G_train, len(D_plus))\n",
    "    D = D_plus + D_minus\n",
    "    df = get_features(GCC, D, 'pickles/sp_train_2013.pkl')\n",
    "    y = df['edge']\n",
    "    X = df.ix[:, df.columns != 'edge']\n",
    "    model.fit(X,y)\n",
    "    preds = model.predict_proba(X_test)\n",
    "    mean_avg_precision += average_precision_score(y_test, preds[:,1])\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, preds[:,1])\n",
    "    plt.plot(recall, precision, lw=1, label='Class {}, Model: {}'.format(1, i))\n",
    "\n",
    "print 'Average precsion: {}'.format(mean_avg_precision / n)\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision and Recall for logistic regression')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)\n",
    "mean_avg_precision += average_precision_score(y_test, preds[:,1])\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, preds[:,1])\n",
    "plt.plot(recall, precision, lw=1, label='Class {}, Model: {}'.format(1, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>common_neighbors</th>\n",
       "      <th>time_difference</th>\n",
       "      <th>triadic_closeness</th>\n",
       "      <th>src_degree</th>\n",
       "      <th>trg_degree</th>\n",
       "      <th>degree_product</th>\n",
       "      <th>common_referrersadamic_adar</th>\n",
       "      <th>leicht_holme_newman</th>\n",
       "      <th>resource_allocation</th>\n",
       "      <th>common_referrers</th>\n",
       "      <th>adamic_adar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(62010CJ0182, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>2926</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0277, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>2919</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0583, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>3171</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0581, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>3176</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0586, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>2905</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0587, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>3150</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0629, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>3157</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0072, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>2926</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0262, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>3129</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0392, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>3150</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0391, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>3157</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0395, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>3227</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0621, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>2996</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0620, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>3003</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0628, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>3080</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0090, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>2954</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0097, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>3024</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0096, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>3129</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0098, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>3024</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62009CJ0348, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>3022</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0017, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>2924</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0669, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>3157</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0600, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>3206</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0602, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>3073</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0357, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>3010</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0604, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>2940</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0607, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>2968</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0354, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>2940</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0157, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>2954</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0048, 62002CJ0230)</th>\n",
       "      <td>0</td>\n",
       "      <td>3080</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0190, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4734</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0192, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4510</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62009CJ0269, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4678</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>217</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0037, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4776</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0035, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4802</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>196</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0034, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4804</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0033, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4685</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0031, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4685</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0445, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4838</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>147</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0441, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4825</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0564, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4573</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0565, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4685</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0566, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4816</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0567, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4566</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0562, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4678</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0393, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4545</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0372, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4531</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0207, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4838</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0019, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4664</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0018, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4734</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0549, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4594</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0544, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4734</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0226, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4832</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0223, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4657</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0229, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4797</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0588, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4510</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0213, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4594</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0210, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4524</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62010CJ0218, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4510</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(62011CJ0592, 61997CJ0124)</th>\n",
       "      <td>0</td>\n",
       "      <td>4783</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000550 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            common_neighbors  time_difference  \\\n",
       "(62010CJ0182, 62002CJ0230)                 0             2926   \n",
       "(62010CJ0277, 62002CJ0230)                 0             2919   \n",
       "(62010CJ0583, 62002CJ0230)                 0             3171   \n",
       "(62010CJ0581, 62002CJ0230)                 0             3176   \n",
       "(62010CJ0586, 62002CJ0230)                 0             2905   \n",
       "(62010CJ0587, 62002CJ0230)                 0             3150   \n",
       "(62011CJ0629, 62002CJ0230)                 0             3157   \n",
       "(62010CJ0072, 62002CJ0230)                 0             2926   \n",
       "(62010CJ0262, 62002CJ0230)                 0             3129   \n",
       "(62011CJ0392, 62002CJ0230)                 0             3150   \n",
       "(62011CJ0391, 62002CJ0230)                 0             3157   \n",
       "(62011CJ0395, 62002CJ0230)                 0             3227   \n",
       "(62010CJ0621, 62002CJ0230)                 0             2996   \n",
       "(62010CJ0620, 62002CJ0230)                 0             3003   \n",
       "(62010CJ0628, 62002CJ0230)                 0             3080   \n",
       "(62011CJ0090, 62002CJ0230)                 0             2954   \n",
       "(62011CJ0097, 62002CJ0230)                 0             3024   \n",
       "(62011CJ0096, 62002CJ0230)                 0             3129   \n",
       "(62011CJ0098, 62002CJ0230)                 0             3024   \n",
       "(62009CJ0348, 62002CJ0230)                 0             3022   \n",
       "(62010CJ0017, 62002CJ0230)                 0             2924   \n",
       "(62011CJ0669, 62002CJ0230)                 0             3157   \n",
       "(62010CJ0600, 62002CJ0230)                 0             3206   \n",
       "(62010CJ0602, 62002CJ0230)                 0             3073   \n",
       "(62010CJ0357, 62002CJ0230)                 0             3010   \n",
       "(62010CJ0604, 62002CJ0230)                 0             2940   \n",
       "(62010CJ0607, 62002CJ0230)                 0             2968   \n",
       "(62010CJ0354, 62002CJ0230)                 0             2940   \n",
       "(62011CJ0157, 62002CJ0230)                 0             2954   \n",
       "(62011CJ0048, 62002CJ0230)                 0             3080   \n",
       "...                                      ...              ...   \n",
       "(62011CJ0190, 61997CJ0124)                 0             4734   \n",
       "(62011CJ0192, 61997CJ0124)                 0             4510   \n",
       "(62009CJ0269, 61997CJ0124)                 0             4678   \n",
       "(62011CJ0037, 61997CJ0124)                 0             4776   \n",
       "(62011CJ0035, 61997CJ0124)                 0             4802   \n",
       "(62011CJ0034, 61997CJ0124)                 0             4804   \n",
       "(62011CJ0033, 61997CJ0124)                 0             4685   \n",
       "(62011CJ0031, 61997CJ0124)                 0             4685   \n",
       "(62011CJ0445, 61997CJ0124)                 0             4838   \n",
       "(62011CJ0441, 61997CJ0124)                 0             4825   \n",
       "(62010CJ0564, 61997CJ0124)                 0             4573   \n",
       "(62010CJ0565, 61997CJ0124)                 0             4685   \n",
       "(62010CJ0566, 61997CJ0124)                 0             4816   \n",
       "(62010CJ0567, 61997CJ0124)                 0             4566   \n",
       "(62010CJ0562, 61997CJ0124)                 0             4678   \n",
       "(62010CJ0393, 61997CJ0124)                 0             4545   \n",
       "(62010CJ0372, 61997CJ0124)                 0             4531   \n",
       "(62011CJ0207, 61997CJ0124)                 0             4838   \n",
       "(62011CJ0019, 61997CJ0124)                 0             4664   \n",
       "(62011CJ0018, 61997CJ0124)                 0             4734   \n",
       "(62010CJ0549, 61997CJ0124)                 0             4594   \n",
       "(62010CJ0544, 61997CJ0124)                 0             4734   \n",
       "(62011CJ0226, 61997CJ0124)                 0             4832   \n",
       "(62011CJ0223, 61997CJ0124)                 0             4657   \n",
       "(62011CJ0229, 61997CJ0124)                 0             4797   \n",
       "(62010CJ0588, 61997CJ0124)                 0             4510   \n",
       "(62010CJ0213, 61997CJ0124)                 0             4594   \n",
       "(62010CJ0210, 61997CJ0124)                 0             4524   \n",
       "(62010CJ0218, 61997CJ0124)                 0             4510   \n",
       "(62011CJ0592, 61997CJ0124)                 0             4783   \n",
       "\n",
       "                            triadic_closeness  src_degree  trg_degree  \\\n",
       "(62010CJ0182, 62002CJ0230)                  0          11           5   \n",
       "(62010CJ0277, 62002CJ0230)                  0           6           5   \n",
       "(62010CJ0583, 62002CJ0230)                  0           4           5   \n",
       "(62010CJ0581, 62002CJ0230)                  0          13           5   \n",
       "(62010CJ0586, 62002CJ0230)                  0           6           5   \n",
       "(62010CJ0587, 62002CJ0230)                  0           8           5   \n",
       "(62011CJ0629, 62002CJ0230)                  0           2           5   \n",
       "(62010CJ0072, 62002CJ0230)                  0          11           5   \n",
       "(62010CJ0262, 62002CJ0230)                  0           4           5   \n",
       "(62011CJ0392, 62002CJ0230)                  0          10           5   \n",
       "(62011CJ0391, 62002CJ0230)                  0           5           5   \n",
       "(62011CJ0395, 62002CJ0230)                  0          10           5   \n",
       "(62010CJ0621, 62002CJ0230)                  0          17           5   \n",
       "(62010CJ0620, 62002CJ0230)                  0           1           5   \n",
       "(62010CJ0628, 62002CJ0230)                  0           9           5   \n",
       "(62011CJ0090, 62002CJ0230)                  0           7           5   \n",
       "(62011CJ0097, 62002CJ0230)                  0           4           5   \n",
       "(62011CJ0096, 62002CJ0230)                  0           4           5   \n",
       "(62011CJ0098, 62002CJ0230)                  0           4           5   \n",
       "(62009CJ0348, 62002CJ0230)                  0           5           5   \n",
       "(62010CJ0017, 62002CJ0230)                  0          23           5   \n",
       "(62011CJ0669, 62002CJ0230)                  0          12           5   \n",
       "(62010CJ0600, 62002CJ0230)                  0           7           5   \n",
       "(62010CJ0602, 62002CJ0230)                  0           6           5   \n",
       "(62010CJ0357, 62002CJ0230)                  0          23           5   \n",
       "(62010CJ0604, 62002CJ0230)                  0           8           5   \n",
       "(62010CJ0607, 62002CJ0230)                  0           3           5   \n",
       "(62010CJ0354, 62002CJ0230)                  0          11           5   \n",
       "(62011CJ0157, 62002CJ0230)                  0          12           5   \n",
       "(62011CJ0048, 62002CJ0230)                  0          15           5   \n",
       "...                                       ...         ...         ...   \n",
       "(62011CJ0190, 61997CJ0124)                  0           3           7   \n",
       "(62011CJ0192, 61997CJ0124)                  0           6           7   \n",
       "(62009CJ0269, 61997CJ0124)                  0          31           7   \n",
       "(62011CJ0037, 61997CJ0124)                  0           3           7   \n",
       "(62011CJ0035, 61997CJ0124)                  0          28           7   \n",
       "(62011CJ0034, 61997CJ0124)                  0           7           7   \n",
       "(62011CJ0033, 61997CJ0124)                  0           8           7   \n",
       "(62011CJ0031, 61997CJ0124)                  0           6           7   \n",
       "(62011CJ0445, 61997CJ0124)                  0          21           7   \n",
       "(62011CJ0441, 61997CJ0124)                  0           0           7   \n",
       "(62010CJ0564, 61997CJ0124)                  0           7           7   \n",
       "(62010CJ0565, 61997CJ0124)                  0           5           7   \n",
       "(62010CJ0566, 61997CJ0124)                  0           4           7   \n",
       "(62010CJ0567, 61997CJ0124)                  0           2           7   \n",
       "(62010CJ0562, 61997CJ0124)                  0          13           7   \n",
       "(62010CJ0393, 61997CJ0124)                  0           9           7   \n",
       "(62010CJ0372, 61997CJ0124)                  0           5           7   \n",
       "(62011CJ0207, 61997CJ0124)                  0           4           7   \n",
       "(62011CJ0019, 61997CJ0124)                  0           3           7   \n",
       "(62011CJ0018, 61997CJ0124)                  0          10           7   \n",
       "(62010CJ0549, 61997CJ0124)                  0          13           7   \n",
       "(62010CJ0544, 61997CJ0124)                  0           8           7   \n",
       "(62011CJ0226, 61997CJ0124)                  0           0           7   \n",
       "(62011CJ0223, 61997CJ0124)                  0           2           7   \n",
       "(62011CJ0229, 61997CJ0124)                  0           6           7   \n",
       "(62010CJ0588, 61997CJ0124)                  0           4           7   \n",
       "(62010CJ0213, 61997CJ0124)                  0           2           7   \n",
       "(62010CJ0210, 61997CJ0124)                  0           6           7   \n",
       "(62010CJ0218, 61997CJ0124)                  0           7           7   \n",
       "(62011CJ0592, 61997CJ0124)                  0           6           7   \n",
       "\n",
       "                            degree_product  common_referrersadamic_adar  \\\n",
       "(62010CJ0182, 62002CJ0230)              55                            0   \n",
       "(62010CJ0277, 62002CJ0230)              30                            0   \n",
       "(62010CJ0583, 62002CJ0230)              20                            0   \n",
       "(62010CJ0581, 62002CJ0230)              65                            0   \n",
       "(62010CJ0586, 62002CJ0230)              30                            0   \n",
       "(62010CJ0587, 62002CJ0230)              40                            0   \n",
       "(62011CJ0629, 62002CJ0230)              10                            0   \n",
       "(62010CJ0072, 62002CJ0230)              55                            0   \n",
       "(62010CJ0262, 62002CJ0230)              20                            0   \n",
       "(62011CJ0392, 62002CJ0230)              50                            0   \n",
       "(62011CJ0391, 62002CJ0230)              25                            0   \n",
       "(62011CJ0395, 62002CJ0230)              50                            0   \n",
       "(62010CJ0621, 62002CJ0230)              85                            0   \n",
       "(62010CJ0620, 62002CJ0230)               5                            0   \n",
       "(62010CJ0628, 62002CJ0230)              45                            0   \n",
       "(62011CJ0090, 62002CJ0230)              35                            0   \n",
       "(62011CJ0097, 62002CJ0230)              20                            0   \n",
       "(62011CJ0096, 62002CJ0230)              20                            0   \n",
       "(62011CJ0098, 62002CJ0230)              20                            0   \n",
       "(62009CJ0348, 62002CJ0230)              25                            0   \n",
       "(62010CJ0017, 62002CJ0230)             115                            0   \n",
       "(62011CJ0669, 62002CJ0230)              60                            0   \n",
       "(62010CJ0600, 62002CJ0230)              35                            0   \n",
       "(62010CJ0602, 62002CJ0230)              30                            0   \n",
       "(62010CJ0357, 62002CJ0230)             115                            0   \n",
       "(62010CJ0604, 62002CJ0230)              40                            0   \n",
       "(62010CJ0607, 62002CJ0230)              15                            0   \n",
       "(62010CJ0354, 62002CJ0230)              55                            0   \n",
       "(62011CJ0157, 62002CJ0230)              60                            0   \n",
       "(62011CJ0048, 62002CJ0230)              75                            0   \n",
       "...                                    ...                          ...   \n",
       "(62011CJ0190, 61997CJ0124)              21                            0   \n",
       "(62011CJ0192, 61997CJ0124)              42                            0   \n",
       "(62009CJ0269, 61997CJ0124)             217                            0   \n",
       "(62011CJ0037, 61997CJ0124)              21                            0   \n",
       "(62011CJ0035, 61997CJ0124)             196                            0   \n",
       "(62011CJ0034, 61997CJ0124)              49                            0   \n",
       "(62011CJ0033, 61997CJ0124)              56                            0   \n",
       "(62011CJ0031, 61997CJ0124)              42                            0   \n",
       "(62011CJ0445, 61997CJ0124)             147                            0   \n",
       "(62011CJ0441, 61997CJ0124)               0                            0   \n",
       "(62010CJ0564, 61997CJ0124)              49                            0   \n",
       "(62010CJ0565, 61997CJ0124)              35                            0   \n",
       "(62010CJ0566, 61997CJ0124)              28                            0   \n",
       "(62010CJ0567, 61997CJ0124)              14                            0   \n",
       "(62010CJ0562, 61997CJ0124)              91                            0   \n",
       "(62010CJ0393, 61997CJ0124)              63                            0   \n",
       "(62010CJ0372, 61997CJ0124)              35                            0   \n",
       "(62011CJ0207, 61997CJ0124)              28                            0   \n",
       "(62011CJ0019, 61997CJ0124)              21                            0   \n",
       "(62011CJ0018, 61997CJ0124)              70                            0   \n",
       "(62010CJ0549, 61997CJ0124)              91                            0   \n",
       "(62010CJ0544, 61997CJ0124)              56                            0   \n",
       "(62011CJ0226, 61997CJ0124)               0                            0   \n",
       "(62011CJ0223, 61997CJ0124)              14                            0   \n",
       "(62011CJ0229, 61997CJ0124)              42                            0   \n",
       "(62010CJ0588, 61997CJ0124)              28                            0   \n",
       "(62010CJ0213, 61997CJ0124)              14                            0   \n",
       "(62010CJ0210, 61997CJ0124)              42                            0   \n",
       "(62010CJ0218, 61997CJ0124)              49                            0   \n",
       "(62011CJ0592, 61997CJ0124)              42                            0   \n",
       "\n",
       "                            leicht_holme_newman  resource_allocation  \\\n",
       "(62010CJ0182, 62002CJ0230)                    0                    0   \n",
       "(62010CJ0277, 62002CJ0230)                    0                    0   \n",
       "(62010CJ0583, 62002CJ0230)                    0                    0   \n",
       "(62010CJ0581, 62002CJ0230)                    0                    0   \n",
       "(62010CJ0586, 62002CJ0230)                    0                    0   \n",
       "(62010CJ0587, 62002CJ0230)                    0                    0   \n",
       "(62011CJ0629, 62002CJ0230)                    0                    0   \n",
       "(62010CJ0072, 62002CJ0230)                    0                    0   \n",
       "(62010CJ0262, 62002CJ0230)                    0                    0   \n",
       "(62011CJ0392, 62002CJ0230)                    0                    0   \n",
       "(62011CJ0391, 62002CJ0230)                    0                    0   \n",
       "(62011CJ0395, 62002CJ0230)                    0                    0   \n",
       "(62010CJ0621, 62002CJ0230)                    0                    0   \n",
       "(62010CJ0620, 62002CJ0230)                    0                    0   \n",
       "(62010CJ0628, 62002CJ0230)                    0                    0   \n",
       "(62011CJ0090, 62002CJ0230)                    0                    0   \n",
       "(62011CJ0097, 62002CJ0230)                    0                    0   \n",
       "(62011CJ0096, 62002CJ0230)                    0                    0   \n",
       "(62011CJ0098, 62002CJ0230)                    0                    0   \n",
       "(62009CJ0348, 62002CJ0230)                    0                    0   \n",
       "(62010CJ0017, 62002CJ0230)                    0                    0   \n",
       "(62011CJ0669, 62002CJ0230)                    0                    0   \n",
       "(62010CJ0600, 62002CJ0230)                    0                    0   \n",
       "(62010CJ0602, 62002CJ0230)                    0                    0   \n",
       "(62010CJ0357, 62002CJ0230)                    0                    0   \n",
       "(62010CJ0604, 62002CJ0230)                    0                    0   \n",
       "(62010CJ0607, 62002CJ0230)                    0                    0   \n",
       "(62010CJ0354, 62002CJ0230)                    0                    0   \n",
       "(62011CJ0157, 62002CJ0230)                    0                    0   \n",
       "(62011CJ0048, 62002CJ0230)                    0                    0   \n",
       "...                                         ...                  ...   \n",
       "(62011CJ0190, 61997CJ0124)                    0                    0   \n",
       "(62011CJ0192, 61997CJ0124)                    0                    0   \n",
       "(62009CJ0269, 61997CJ0124)                    0                    0   \n",
       "(62011CJ0037, 61997CJ0124)                    0                    0   \n",
       "(62011CJ0035, 61997CJ0124)                    0                    0   \n",
       "(62011CJ0034, 61997CJ0124)                    0                    0   \n",
       "(62011CJ0033, 61997CJ0124)                    0                    0   \n",
       "(62011CJ0031, 61997CJ0124)                    0                    0   \n",
       "(62011CJ0445, 61997CJ0124)                    0                    0   \n",
       "(62011CJ0441, 61997CJ0124)                    0                    0   \n",
       "(62010CJ0564, 61997CJ0124)                    0                    0   \n",
       "(62010CJ0565, 61997CJ0124)                    0                    0   \n",
       "(62010CJ0566, 61997CJ0124)                    0                    0   \n",
       "(62010CJ0567, 61997CJ0124)                    0                    0   \n",
       "(62010CJ0562, 61997CJ0124)                    0                    0   \n",
       "(62010CJ0393, 61997CJ0124)                    0                    0   \n",
       "(62010CJ0372, 61997CJ0124)                    0                    0   \n",
       "(62011CJ0207, 61997CJ0124)                    0                    0   \n",
       "(62011CJ0019, 61997CJ0124)                    0                    0   \n",
       "(62011CJ0018, 61997CJ0124)                    0                    0   \n",
       "(62010CJ0549, 61997CJ0124)                    0                    0   \n",
       "(62010CJ0544, 61997CJ0124)                    0                    0   \n",
       "(62011CJ0226, 61997CJ0124)                    0                    0   \n",
       "(62011CJ0223, 61997CJ0124)                    0                    0   \n",
       "(62011CJ0229, 61997CJ0124)                    0                    0   \n",
       "(62010CJ0588, 61997CJ0124)                    0                    0   \n",
       "(62010CJ0213, 61997CJ0124)                    0                    0   \n",
       "(62010CJ0210, 61997CJ0124)                    0                    0   \n",
       "(62010CJ0218, 61997CJ0124)                    0                    0   \n",
       "(62011CJ0592, 61997CJ0124)                    0                    0   \n",
       "\n",
       "                            common_referrers  adamic_adar  \n",
       "(62010CJ0182, 62002CJ0230)                 0            0  \n",
       "(62010CJ0277, 62002CJ0230)                 0            0  \n",
       "(62010CJ0583, 62002CJ0230)                 0            0  \n",
       "(62010CJ0581, 62002CJ0230)                 0            0  \n",
       "(62010CJ0586, 62002CJ0230)                 0            0  \n",
       "(62010CJ0587, 62002CJ0230)                 0            0  \n",
       "(62011CJ0629, 62002CJ0230)                 0            0  \n",
       "(62010CJ0072, 62002CJ0230)                 0            0  \n",
       "(62010CJ0262, 62002CJ0230)                 0            0  \n",
       "(62011CJ0392, 62002CJ0230)                 0            0  \n",
       "(62011CJ0391, 62002CJ0230)                 0            0  \n",
       "(62011CJ0395, 62002CJ0230)                 0            0  \n",
       "(62010CJ0621, 62002CJ0230)                 0            0  \n",
       "(62010CJ0620, 62002CJ0230)                 0            0  \n",
       "(62010CJ0628, 62002CJ0230)                 0            0  \n",
       "(62011CJ0090, 62002CJ0230)                 0            0  \n",
       "(62011CJ0097, 62002CJ0230)                 0            0  \n",
       "(62011CJ0096, 62002CJ0230)                 0            0  \n",
       "(62011CJ0098, 62002CJ0230)                 0            0  \n",
       "(62009CJ0348, 62002CJ0230)                 0            0  \n",
       "(62010CJ0017, 62002CJ0230)                 0            0  \n",
       "(62011CJ0669, 62002CJ0230)                 0            0  \n",
       "(62010CJ0600, 62002CJ0230)                 0            0  \n",
       "(62010CJ0602, 62002CJ0230)                 0            0  \n",
       "(62010CJ0357, 62002CJ0230)                 0            0  \n",
       "(62010CJ0604, 62002CJ0230)                 0            0  \n",
       "(62010CJ0607, 62002CJ0230)                 0            0  \n",
       "(62010CJ0354, 62002CJ0230)                 0            0  \n",
       "(62011CJ0157, 62002CJ0230)                 0            0  \n",
       "(62011CJ0048, 62002CJ0230)                 0            0  \n",
       "...                                      ...          ...  \n",
       "(62011CJ0190, 61997CJ0124)                 0            0  \n",
       "(62011CJ0192, 61997CJ0124)                 0            0  \n",
       "(62009CJ0269, 61997CJ0124)                 1            0  \n",
       "(62011CJ0037, 61997CJ0124)                 0            0  \n",
       "(62011CJ0035, 61997CJ0124)                 3            0  \n",
       "(62011CJ0034, 61997CJ0124)                 0            0  \n",
       "(62011CJ0033, 61997CJ0124)                 0            0  \n",
       "(62011CJ0031, 61997CJ0124)                 0            0  \n",
       "(62011CJ0445, 61997CJ0124)                 0            0  \n",
       "(62011CJ0441, 61997CJ0124)                 0            0  \n",
       "(62010CJ0564, 61997CJ0124)                 0            0  \n",
       "(62010CJ0565, 61997CJ0124)                 0            0  \n",
       "(62010CJ0566, 61997CJ0124)                 1            0  \n",
       "(62010CJ0567, 61997CJ0124)                 0            0  \n",
       "(62010CJ0562, 61997CJ0124)                 1            0  \n",
       "(62010CJ0393, 61997CJ0124)                 0            0  \n",
       "(62010CJ0372, 61997CJ0124)                 0            0  \n",
       "(62011CJ0207, 61997CJ0124)                 0            0  \n",
       "(62011CJ0019, 61997CJ0124)                 0            0  \n",
       "(62011CJ0018, 61997CJ0124)                 0            0  \n",
       "(62010CJ0549, 61997CJ0124)                 0            0  \n",
       "(62010CJ0544, 61997CJ0124)                 0            0  \n",
       "(62011CJ0226, 61997CJ0124)                 0            0  \n",
       "(62011CJ0223, 61997CJ0124)                 0            0  \n",
       "(62011CJ0229, 61997CJ0124)                 0            0  \n",
       "(62010CJ0588, 61997CJ0124)                 0            0  \n",
       "(62010CJ0213, 61997CJ0124)                 0            0  \n",
       "(62010CJ0210, 61997CJ0124)                 0            0  \n",
       "(62010CJ0218, 61997CJ0124)                 0            0  \n",
       "(62011CJ0592, 61997CJ0124)                 0            0  \n",
       "\n",
       "[3000550 rows x 11 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefs:     1.5208    -0.0003     1.2668     0.0499    -0.0246    -0.0031     2.4502 \n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    c_str = \"\"\n",
    "    for coef in model.coef_[0]:\n",
    "        c_str += '{:10.4f} '.format(coef)\n",
    "    print 'Coefs: ' + c_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(X_test.loc[y_test==0, 'time_difference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print classification_report(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = classifier.predict_proba(X_test)\n",
    "average_precision = average_precision_score(y_test, preds[:,1])\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, preds[:,1])\n",
    "plt.plot(recall, precision, lw=1, label='Class {}, avg. precision: {}'.format(1, average_precision))\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision and Recall for logistic regression')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('pickles/sp_train_2013.pkl', 'wb') as fl:\n",
    "    pickle.dump(df.loc[:, 'shortest_path'], fl)\n",
    "with open('pickles/sp_test_2013.pkl', 'wb') as fl:\n",
    "    pickle.dump(test_data.loc[:, 'shortest_path'], fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the accuracy of the logistic model seems incredibly good this simple evaluation hides an important issue which can be seen when looking at the per-class metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "preds = logi.predict(test_data[:,1:])\n",
    "print classification_report(test_data[:,0], preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The model is able to predict the non-existence of links very well, but completely fails to predict existing links which are exactly the cases that are interesting. This can be seen as having a model that will always predict that no link will form will have very nice accuracy due to the imbalance between classes. It is therefore necessary to train the model on an a more balanced dataset.\n",
    "\n",
    "[Link prediction in dynamic weighted and directed social networks] proposes a way to combat this. Since the set of potential edges, $D$, has the subsets of existing links, $D^+$ and non-existing links, $D^-$, where $D^- >> D^+$ it is possible to split into $m$ sub-sets each of size $|D^+|$, $S_i$ so that $\\sum_i S_i = D^-$.\n",
    "\n",
    "Models are then trained on $D^+ S_i \\ \\forall \\ i \\in m$  resulting in $m$ models all trained on the same positive cases. To perform classification of a potential link majority voting among the classifiers is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the cases in positive and negative samples\n",
    "D_pos = train_data[train_data[:,0] == 1]\n",
    "D_neg = train_data[train_data[:,0] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute models for each case\n",
    "m = len(D_neg)/len(D_pos)\n",
    "try:\n",
    "    models = pkl.load(open('pickles/majority_logistic.pkl', 'rb'))\n",
    "except:\n",
    "    models = []\n",
    "    for i in range(0, m):\n",
    "        S = np.array(D_neg[i*len(D_pos):(i+1)*len(D_pos)])\n",
    "        model = LogisticRegression(C=1e5)\n",
    "        models.append(model.fit(np.append(S[:,1:], D_pos[:,1:], axis=0), np.append(S[:,0], D_pos[:,0], axis = 0)))\n",
    "    with open('pickles/majority_logistic.pkl', 'wb') as fl:\n",
    "        pkl.dump(models, fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute models for each case\n",
    "m = len(D_neg)/len(D_pos)\n",
    "try:\n",
    "    models = pkl.load(open('pickles/majority_svc.pkl', 'rb'))\n",
    "except:\n",
    "    models = []\n",
    "    for i in range(0, m):\n",
    "        S = np.array(D_neg[i*len(D_pos):(i+1)*len(D_pos)])\n",
    "        model = LinearSVC()\n",
    "        models.append(model.fit(np.append(S[:,1:], D_pos[:,1:], axis=0), np.append(S[:,0], D_pos[:,0], axis = 0)))\n",
    "    with open('pickles/majority_svc.pkl', 'wb') as fl:\n",
    "        pkl.dump(models, fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create predictions for each model and decide the final outcome by majority voting\n",
    "try:\n",
    "    predictions = pkl.load(open('pickles/majority_pred.pkl', 'rb'))\n",
    "except:\n",
    "    predictions = np.zeros(test_data.shape[0])\n",
    "    for i in range(0, len(models)):\n",
    "        predictions += models[i].predict(test_data[:, 1:])\n",
    "    with open('pickles/majority_pred.pkl', 'wb') as fl:\n",
    "        pkl.dump(predictions, fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create predictions for each model and decide the final outcome by majority voting\n",
    "try:\n",
    "    svc_pred = pkl.load(open('pickles/majority_svc_pred.pkl', 'rb'))\n",
    "except:\n",
    "    svc_pred = np.zeros(test_data.shape[0])\n",
    "    for i in range(0, len(models)):\n",
    "        svc_pred += models[i].predict(test_data[:, 1:])\n",
    "    with open('pickles/majority_svc_pred.pkl', 'wb') as fl:\n",
    "        pkl.dump(svc_pred, fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voted_result = np.zeros(len(svc_pred))\n",
    "for i, agg in enumerate(svc_pred):\n",
    "    if agg > m/2:\n",
    "        voted_result[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "voted_result = np.zeros(len(predictions))\n",
    "for i, agg in enumerate(predictions):\n",
    "    if agg > m/2:\n",
    "        voted_result[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print classification_report(test_data[:,0], voted_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodes = slice_graph_by_year(start_date, start_date-7, GCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = cv.train_test_split(train_data[:,1:], train_data[:,0], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "2142685L*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = cv.cross_val_score(ll, train_data[:,1:], train_data[:,0], cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a cross validation experiment on the logistic classifier based on the years 2000 - 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.clock()\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score, make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "# For now just use the pre-calculated data\n",
    "n=2\n",
    "# Reasonable parameters to be tuned\n",
    "tuned_parameters = [{'class_weight': [{0: 1.0/alpha, 1:1.0} for alpha in [4000.0, 5000.0, 6000.0, 10000.0, 10000.0]]}]\n",
    "data = train_data\n",
    "y = data[:,0]\n",
    "X = data[:,1:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=0)\n",
    "scorer = lambda ground_truth, predictions: average_precision_score(ground_truth, predictions[:,1])\n",
    "print(\"# Tuning hyper-parameters for AUC-PR\")\n",
    "print()\n",
    "\n",
    "clf = GridSearchCV(LogisticRegression(), tuned_parameters, cv=5,\n",
    "                   scoring=make_scorer(scorer, needs_proba=True))\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "for params, mean_score, scores in clf.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean_score, scores.std() * 2, params))\n",
    "print()\n",
    "\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "print(\"The model is trained on the full development set.\")\n",
    "print(\"The scores are computed on the full evaluation set.\")\n",
    "print()\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()\n",
    "t1 = time.clock()\n",
    "\n",
    "total = t1-t0\n",
    "print \"Total time used: {}\".format(total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = train_data\n",
    "y = data[:,0]\n",
    "X = data[:,1:]\n",
    "alphas = [1.0/1.0]\n",
    "n = 2\n",
    "models = [('Logistic Regression', LogisticRegression(C=1e5)), ('Decision tree', tree.DecisionTreeClassifier())]\n",
    "for model_name, classifier in models:\n",
    "    cv = StratifiedKFold(y, n_folds=n)\n",
    "    n_classes = 2\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    average_precision = {0:0.0, 1:0.0}\n",
    "\n",
    "    for fold, (train, test) in enumerate(cv):\n",
    "        probas = classifier.fit(X[train], y[train]).predict_proba(X[test])\n",
    "        # Compute average precision\n",
    "        for i in range(n_classes):\n",
    "            average_precision[i] += average_precision_score(y[test], probas[:,i])\n",
    "    i = 1\n",
    "    average_precision[i] /= n\n",
    "    precision[i], recall[i], thresholds = precision_recall_curve(y[test], probas[:, i])\n",
    "    plt.plot(recall[i], precision[i], lw=1, label='Class {}, avg. precision: {}, Model: {}'.format(i, average_precision[i], model_name))\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision and Recall for logistic regression')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print classification_report(y[test], classifier.predict(X[test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.tree_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
