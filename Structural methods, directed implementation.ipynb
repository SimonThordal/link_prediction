{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run load_data.py\n",
    "import sys\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import scipy.sparse.linalg as splin\n",
    "import scipy.sparse as sparse\n",
    "import random\n",
    "import math\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document the methods described in the methods section are implemented for the network of verdicts. The data is loaded as `networkx`directed graph making it relatively easy to work with. The goal is to set up an easily used interface for running K-folds cross validation on the network for different link prediction algorithms and evaluate them with ROC and precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the greatest connected component and work on that\n",
    "components = []\n",
    "lengths = []\n",
    "# Find the greatest component from the undirected version of the graph\n",
    "for component in nx.connected_component_subgraphs(nx.Graph(G)):\n",
    "    components.append(component)\n",
    "    lengths.append(len(component))\n",
    "# Find the GCC as the largest component and then recreate the directed graph\n",
    "GCC = components[lengths.index(max(lengths))]\n",
    "GCC = G.subgraph(GCC.nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is an undirected implementation we'll need an undirected version of the GCC. Changing to undirected has a tendency to swap edge the position nodes on edges. Position is necessary to trim the graph, so we'll make sure that `edge[0]` is always younger than `edge[1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an undirected copy of the GCC and make sure that the edges in the copy respect causality\n",
    "GCC_un = nx.Graph(GCC.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def jaccard(validation_set, G):\n",
    "    \"\"\"\n",
    "    Perform Jaccard scoring on a single edge\n",
    "    \n",
    "    arguments:\n",
    "    non_edge -- edge tuple specified by node endpoints\n",
    "    G -- graph containing the nodes in the edge\n",
    "    \n",
    "    return:\n",
    "    edges with the score as an attribute\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for non_edge in validation_set:\n",
    "        u = set(G.neighbors(non_edge[0]))\n",
    "        v = set(G.neighbors(non_edge[1]))\n",
    "        uv_un = 1.0*len(u.union(v))\n",
    "        uv_int = 1.0*len(u.intersection(v))\n",
    "        if uv_int == 0 or uv_un == 0:\n",
    "            s= 0.0\n",
    "        else:\n",
    "            s = uv_int/uv_un\n",
    "        non_edge[2]['score'] = s\n",
    "        results.append(non_edge)\n",
    "    return results\n",
    "    \n",
    "def common_neighbors(validation_set, G):\n",
    "    \"\"\"\n",
    "    Perform common neighbors scoring on a single edge\n",
    "    \n",
    "    arguments:\n",
    "    non_edge -- edge tuple specified by node endpoints\n",
    "    G -- graph containing the nodes in the edge\n",
    "    \n",
    "    return:\n",
    "    edges with the score as an attribute\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for non_edge in validation_set:\n",
    "        u = set(G.neighbors(non_edge[0]))\n",
    "        v = set(G.neighbors(non_edge[1]))\n",
    "        s = len(u.intersection(v))\n",
    "        non_edge[2]['score'] = s\n",
    "        results.append(non_edge)\n",
    "    return results\n",
    "\n",
    "def adamic_adar(validation_set, G):\n",
    "    \"\"\"\n",
    "    Perform Adamic/Adar scoring on a single edge\n",
    "    \n",
    "    arguments:\n",
    "    non_edge -- edge tuple specified by node endpoints\n",
    "    G -- graph containing the nodes in the edge\n",
    "    \n",
    "    return:\n",
    "    edges with the score as an attribute\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for non_edge in validation_set:\n",
    "        u = set(G.neighbors(non_edge[0]))\n",
    "        v = set(G.neighbors(non_edge[1]))\n",
    "        uv_int = u.intersection(v)\n",
    "        s = sum([1.0/math.log(G.degree(node)) for node in uv_int if G.degree(node) != 1 and G.degree(node) != 0])\n",
    "        if s == None:\n",
    "            s = 0.0\n",
    "        non_edge[2]['score'] = s\n",
    "        results.append(non_edge)\n",
    "    return results\n",
    "\n",
    "def resource_allocation(validation_set, G):\n",
    "    \"\"\"\n",
    "    Perform resource allocation scoring on a single edge\n",
    "    \n",
    "    arguments:\n",
    "    non_edge -- edge tuple specified by node endpoints\n",
    "    G -- graph containing the nodes in the edge\n",
    "    \n",
    "    return:\n",
    "    edges with the score as an attribute\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for non_edge in validation_set:\n",
    "        u = set(G.neighbors(non_edge[0]))\n",
    "        v = set(G.neighbors(non_edge[1]))\n",
    "        uv_int = u.intersection(v)\n",
    "        s = sum([1.0/G.degree(node) for node in uv_int if G.degree(node) != 0])\n",
    "        non_edge[2]['score'] = s\n",
    "        results.append(non_edge)\n",
    "    return results\n",
    "\n",
    "def leicht_holme_newman(validation_set, G):\n",
    "    \"\"\"\n",
    "    Perform LHN1 scoring on a single edge\n",
    "    \n",
    "    arguments:\n",
    "    non_edge -- edge tuple specified by node endpoints\n",
    "    G -- graph containing the nodes in the edge\n",
    "    \n",
    "    return:\n",
    "    edges with the score as an attribute\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for non_edge in validation_set:\n",
    "        u = set(G.neighbors(non_edge[0]))\n",
    "        v = set(G.neighbors(non_edge[1]))\n",
    "        uv_int = 1.0*len(u.intersection(v))\n",
    "        if G.degree(non_edge[0]) == 0 or G.degree(non_edge[1]) == 0:\n",
    "            s = 0.0\n",
    "        else:\n",
    "            s = uv_int/(G.degree(non_edge[0])*G.degree(non_edge[1]))\n",
    "        non_edge[2]['score'] = s\n",
    "        results.append(non_edge)\n",
    "    return results\n",
    "\n",
    "def sorensen(validation_set, G):\n",
    "    \"\"\"\n",
    "    Perform SÃ¸rensen index scoring on a single edge\n",
    "    \n",
    "    arguments:\n",
    "    non_edge -- edge tuple specified by node endpoints\n",
    "    G -- graph containing the nodes in the edge\n",
    "    \n",
    "    return:\n",
    "    edges with the score as an attribute\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for non_edge in validation_set:\n",
    "        u = set(G.neighbors(non_edge[0]))\n",
    "        v = set(G.neighbors(non_edge[1]))\n",
    "        uv_int = 1.0*len(u.intersection(v))\n",
    "        if G.degree(non_edge[0]) == 0 and G.degree(non_edge[1]) == 0:\n",
    "            s = 0.0\n",
    "        else:\n",
    "            s = 2.0*uv_int/(G.degree(non_edge[0])+G.degree(non_edge[1]))\n",
    "        non_edge[2]['score'] = s\n",
    "        results.append(non_edge)\n",
    "    return results\n",
    "\n",
    "def hub_promoted(validation_set, G):\n",
    "    \"\"\"\n",
    "    Perform hub promoted index scoring on a single edge\n",
    "    \n",
    "    arguments:\n",
    "    non_edge -- edge tuple specified by node endpoints\n",
    "    G -- graph containing the nodes in the edge\n",
    "    \n",
    "    return:\n",
    "    edges with the score as an attribute\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for non_edge in validation_set:\n",
    "        u = set(G.neighbors(non_edge[0]))\n",
    "        v = set(G.neighbors(non_edge[1]))\n",
    "        uv_int = 1.0*len(u.intersection(v))\n",
    "        if G.degree(non_edge[0]) == 0 and G.degree(non_edge[1]) == 0:\n",
    "            s = 0.0\n",
    "        else:\n",
    "            s = uv_int/max(G.degree(non_edge[0]), G.degree(non_edge[1]))\n",
    "        non_edge[2]['score'] = s\n",
    "        results.append(non_edge)\n",
    "    return results\n",
    "\n",
    "def hub_depressed(validation_set, G):\n",
    "    \"\"\"\n",
    "    Perform hub promoted index scoring on a single edge\n",
    "    \n",
    "    arguments:\n",
    "    non_edge -- edge tuple specified by node endpoints\n",
    "    G -- graph containing the nodes in the edge\n",
    "    \n",
    "    return:\n",
    "    edges with the score as an attribute\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for non_edge in validation_set:\n",
    "        u = set(G.neighbors(non_edge[0]))\n",
    "        v = set(G.neighbors(non_edge[1]))\n",
    "        uv_int = 1.0*len(u.intersection(v))\n",
    "        if G.degree(non_edge[0]) == 0 or G.degree(non_edge[1]) == 0:\n",
    "            s = 0.0\n",
    "        else:\n",
    "            s = uv_int/min(G.degree(non_edge[0]), G.degree(non_edge[1]))\n",
    "        non_edge[2]['score'] = s\n",
    "        results.append(non_edge)\n",
    "    return results\n",
    "\n",
    "def salton(validation_set, G):\n",
    "    \"\"\"\n",
    "    Perform Salton index scoring on a single edge\n",
    "    \n",
    "    arguments:\n",
    "    non_edge -- edge tuple specified by node endpoints\n",
    "    G -- graph containing the nodes in the edge\n",
    "    \n",
    "    return:\n",
    "    edges with the score as an attribute\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for non_edge in validation_set:\n",
    "        u = set(G.neighbors(non_edge[0]))\n",
    "        v = set(G.neighbors(non_edge[1]))\n",
    "        uv_int = 1.0*len(u.intersection(v))\n",
    "        if G.degree(non_edge[0]) == 0 or G.degree(non_edge[1]) == 0:\n",
    "            s = 0.0\n",
    "        else:\n",
    "            s = uv_int/math.sqrt(G.degree(non_edge[0])*G.degree(non_edge[1]))\n",
    "        non_edge[2]['score'] = s\n",
    "        results.append(non_edge)\n",
    "    return results\n",
    "\n",
    "def preferential_attachment(validation_set, G):\n",
    "    \"\"\"\n",
    "    Perform preferential attachment scoring on a single edge\n",
    "    \n",
    "    arguments:\n",
    "    non_edge -- edge tuple specified by node endpoints\n",
    "    G -- graph containing the nodes in the edge\n",
    "    \n",
    "    return:\n",
    "    edges with the score as an attribute\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for non_edge in validation_set:\n",
    "        s = G.degree(non_edge[0])*G.degree(non_edge[1])\n",
    "        non_edge[2]['score'] = s\n",
    "        results.append(non_edge)\n",
    "    return results\n",
    "\n",
    "## Global index similarities, computed based on every path between two nodes. SLower than local indices but contain full\n",
    "## information\n",
    "def katz_index(validation_set, G, beta):\n",
    "    \"\"\"\n",
    "    Return a weighted sum over every path between two nodes, s_xy = sum(beta*A+beta^2*A^2+...) = (I-beta*A^(-1))-I\n",
    "    \n",
    "    arguments:\n",
    "    validation_set -- set of edges between nodes to be considered\n",
    "    A_inv -- Precomputed inverse of the adjacency matrix\n",
    "    beta -- weighting parameter, the smaller it is the closer the result is to CN\n",
    "    \n",
    "    return:\n",
    "    list of edges with the score as an attribute\n",
    "    \"\"\"\n",
    "    A = nx.adjacency_matrix(G)\n",
    "    # Create indices to access A\n",
    "    id_to_ind = {node: i for (i, node) in zip(range(0, len(G.nodes())), G.nodes())}\n",
    "    max_eig = splin.eigsh(A.asfptype(), k=1, which='LM', return_eigenvectors=False)[0]\n",
    "    if 1.0/max_eig <= beta:\n",
    "        raise Exception(\"Beta must be less than or equal to the maximum eigenvalue of A(G), which is: {}\".format(1.0/max_eig))\n",
    "    \n",
    "    I = np.identity(A.shape[0])\n",
    "    S = np.linalg.inv(I-beta*A.todense()) - I\n",
    "    results = [(x,y, {'score': S[id_to_ind[x], id_to_ind[y]]}) for (x,y) in validation_set]\n",
    "    return results\n",
    "\n",
    "def leicht_holme_newman_global(validation_set, G, alpha):\n",
    "    \"\"\"\n",
    "    Return a weighted sum of every path between two nodes, s_xy = sum(I+omega*A+omega^2*A^2+...)\n",
    "    \"\"\"\n",
    "    A = nx.adjacency_matrix(G)\n",
    "    # Create indices to access A\n",
    "    id_to_ind = {node: i for (i, node) in zip(range(0, len(G.nodes())), G.nodes())}\n",
    "    M = len(G.edges())\n",
    "    max_eig = splin.eigsh(A.asfptype(), k=1, which='LM', return_eigenvectors=False)[0]\n",
    "    I = np.identity(A.shape[0])\n",
    "    D_inv = np.linalg.inv(I+np.diag(G.degree().values())) # TODO: This normally does not have the I factor, but this is done to remove 0-rows creating singular matrices\n",
    "    A_inv=np.linalg.inv((I-alpha*A)/max_eig)\n",
    "    S = 2*M*max_eig*D_inv*A_inv*D_inv\n",
    "    results = [(x,y, {'score': S[id_to_ind[x], id_to_ind[y]]}) for (x,y) in validation_set]\n",
    "    return results\n",
    "\n",
    "def average_commute_time(validation_set, G):\n",
    "    \"\"\"\n",
    "    Return the score as the inverse of the average commute time between two nodes, found as the pseudo inverse of the Laplacian\n",
    "    \"\"\"\n",
    "    A = nx.adjacency_matrix(G)\n",
    "    # Create indices to access A\n",
    "    id_to_ind = {node: i for (i, node) in zip(range(0, len(G.nodes())), G.nodes())}\n",
    "    L_plus = np.linalg.pinv(nx.laplacian_matrix(G).todense())\n",
    "    results = []\n",
    "    scoring = lambda x,y: 1.0/(L_plus[id_to_ind[x], id_to_ind[x]] + L_plus[id_to_ind[y], id_to_ind[y]] - 2*L_plus[id_to_ind[x], id_to_ind[y]])\n",
    "    for x,y in validation_set:\n",
    "        results.append((x,y, {'score': scoring(x,y)}))\n",
    "    return results\n",
    "\n",
    "def local_path_index(validation_set, G, epsilon, n):\n",
    "    \"\"\"\n",
    "    The same as the katz index, but constricted to paths of length n\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for x,y in validation_set:\n",
    "        count = [0 for _ in range(0, n+1)]\n",
    "        q = [(x, 0)]\n",
    "        while len(q) != 0:\n",
    "            v, dist = q.pop()\n",
    "            for w in G.neighbors(v):\n",
    "                if dist < (n-1):\n",
    "                    q.append((w, dist + 1))\n",
    "                if dist < n and w == y:\n",
    "                    count[dist] += 1\n",
    "        s = sum([math.pow(epsilon,1.0*i)*c for i, c in zip(range(0, len(count) + 1), count)])\n",
    "        results.append((x,y, {'score': s}))\n",
    "    return results\n",
    "\n",
    "def local_path_index2(validation_set, G, epsilon, n):\n",
    "    \"\"\"\n",
    "    The same as the katz index, but constricted to paths of length n\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for x,y in validation_set:\n",
    "        s = 0.0\n",
    "        neighborhood = set(G.neighbors(x))\n",
    "        for i in range(2, n+2):\n",
    "            if y in neighborhood:\n",
    "                H = G.subgraph(neighborhood)\n",
    "                id_to_ind = {node: i for (i, node) in zip(range(0, len(H.nodes())), H.nodes())}\n",
    "                A = np.linalg.matrix_power(nx.adjacency_matrix(H).todense(), i)\n",
    "                s += math.pow(epsilon, i-2)*A[id_to_ind[x], id_to_ind[y]]\n",
    "            \n",
    "            for neighbor in list(neighborhood):\n",
    "                neighborhood.update(set(G.neighbors(neighbor)))\n",
    "        results.append((x,y,{'score': s}))\n",
    "    return results\n",
    "\n",
    "def local_random_walk(validation_set, G, t):\n",
    "    \"\"\"\n",
    "    \n",
    "    arguments:\n",
    "    validation_set -- set of edges between nodes to be considered\n",
    "    G -- the graph the validation set is evaluated against\n",
    "    n -- number of steps to be considered\n",
    "    \n",
    "    return:\n",
    "    list of edges with the score as an attribute\n",
    "    \"\"\"\n",
    "    for x,y in validation_set:\n",
    "        pi_xy = 0\n",
    "        pi_yx = 0\n",
    "        for tau in range(1, t+1):\n",
    "            d_x = 0\n",
    "\n",
    "def non_edges(graph):\n",
    "    \"\"\"\n",
    "    Returns the non-existent edges in the graph in a randomized order.\n",
    "    Monkeypatch of the original networkx implementation and will implode under strain due to not using iterators\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph : NetworkX graph.\n",
    "        Graph to find non-existent edges.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    non_edges : iterator\n",
    "        Iterator of edges that are not in the graph.\n",
    "    \"\"\"\n",
    "    if graph.is_directed():\n",
    "        nodes = graph.nodes()\n",
    "        random.shuffle(nodes)\n",
    "        for u in nodes:\n",
    "            for v in nx.non_neighbors(graph, u):\n",
    "                yield (u, v)\n",
    "    else:\n",
    "        nodes = list(set(graph))\n",
    "        random.shuffle(nodes)\n",
    "        nodes = set(nodes)\n",
    "        while nodes:\n",
    "            u = nodes.pop()\n",
    "            for v in nodes - set(graph[u]):\n",
    "                yield (u, v)\n",
    "                \n",
    "def valid_random_non_edges(graph, n):\n",
    "    \"\"\"\n",
    "    Returns randomized, non-existent links between nodes in the graph that are guaranteed to observe causality.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph : NetworkX graph.\n",
    "        Graph to find non-existent edges.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    non_edges : iterator\n",
    "        Iterator of edges that are not in the graph.\n",
    "    \"\"\"\n",
    "    result_pairs = []\n",
    "    # Sort edges according to age\n",
    "    sorted_edges =[node for node, data in sorted(graph.nodes(data=True), key=lambda x: x[1]['date'], reverse=True)]\n",
    "    node_set = set(graph.nodes())\n",
    "    candidates = list(np.random.choice(sorted_edges, n, replace=True))\n",
    "    i = 0\n",
    "    while i < len(candidates):\n",
    "        u = candidates[i]\n",
    "        # Make sure the potential neighbors respect causality with a resolution equal to the timestamp\n",
    "        cand_index = sorted_edges.index(u)\n",
    "        potential_neighbors = set(sorted_edges[cand_index:])\n",
    "        if graph.is_directed():\n",
    "            neighbors = set(graph.successors(u)).union(set(graph.predecessors(u)))\n",
    "        else:\n",
    "            neighbors = set(graph.neighbors(u))\n",
    "        # Make sure the potential neighbors respect causality\n",
    "        non_neighbors = list(potential_neighbors - neighbors)\n",
    "        # The oldest node will have a neighborhood of Ã, so add a new candidate to the list in that case\n",
    "        if len(non_neighbors) == 0:\n",
    "            candidates.append(random.choice(graph.nodes()))\n",
    "        else:    \n",
    "            result_pairs.append((u, random.choice(non_neighbors)))\n",
    "        i += 1\n",
    "    return result_pairs\n",
    "        \n",
    "\n",
    "def k_fold_validate(G, k, fun, **kwargs):\n",
    "    \"\"\"\n",
    "    K-fold validation of some specified function\n",
    "    \n",
    "    arguments:\n",
    "    G -- Graph to perform the function on\n",
    "    k -- number of folds\n",
    "    fun -- function to be evaluated\n",
    "    kwargs -- arguments to be passed to the evaluated function\n",
    "    \n",
    "    return:\n",
    "    List of lists of scored predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    edges = G.edges(data=True)\n",
    "    random.shuffle(edges)\n",
    "    \n",
    "    # Find the number of true members in the validation set\n",
    "    N = len(edges)/k\n",
    "    validation_sets = []\n",
    "    for i in range(0,k):\n",
    "        validation_sets.append(edges[i*N:(i+1)*N])\n",
    "    results = []\n",
    "    for true_validation_set in validation_sets:\n",
    "        # Create a training set and remove all true members of the validation set from it\n",
    "        G_train = G.copy()\n",
    "        G_train.remove_edges_from(true_validation_set)\n",
    "        \n",
    "        # Fetch random edges guaranteed not to be in the graph\n",
    "        false_validation_set = valid_random_non_edges(G, len(true_validation_set))\n",
    "        # Trim out and replace edges if they break causality. Continue doing this until the set is the size of the validation set\n",
    "        for i in range(len(false_validation_set)):\n",
    "            edge = false_validation_set[i]\n",
    "            # Add date information to the non-edge\n",
    "            false_validation_set[i] = edge + ({'date': G.node[edge[0]]['date']},)\n",
    "        \n",
    "        validation_set = true_validation_set + false_validation_set\n",
    "        # If not shuffled, subsequent sorting algorithms will always rank true edges higher than false edges when they have\n",
    "        # the same score\n",
    "        random.shuffle(validation_set)\n",
    "        results.append(fun(validation_set, G_train, **kwargs))\n",
    "    return results\n",
    "\n",
    "def precision(G, results, L):\n",
    "    \"\"\"\n",
    "    Find the ratio of the true positives to trues\n",
    "    \n",
    "    arguments:\n",
    "    G -- graph the results are based on\n",
    "    results -- list of lists of scored predictions\n",
    "    L -- number of results to be considered\n",
    "    \n",
    "    return:\n",
    "    List of precisions for each set of results\n",
    "    \"\"\"\n",
    "    # Sort the results with descending scores\n",
    "    results = [sorted(result, key=lambda x: x[2]['score'], reverse=True) for result in results]\n",
    "    edge_set = set(G.edges())\n",
    "    # True positives exist in both the edge set and the result set\n",
    "    true_positives = [[(edge[0],edge[1]) for edge in result[0:L] if (edge[0], edge[1]) in edge_set] for result in results]\n",
    "    return [1.0*len(trues)/L for trues in true_positives]\n",
    "\n",
    "def AUC(G, results):\n",
    "    \"\"\"\n",
    "    Perform n trials where the score of a non-edge and an edge in the result is compared. Count the number of trials where\n",
    "    the edge had the higher score as n' and the number of times the score was equal as n'' and return the AUC as (n' + n'')/n.\n",
    "    \n",
    "    arguments:\n",
    "    G -- graph the results are based on\n",
    "    results -- list of lists of scored predictions\n",
    "    \n",
    "    return:\n",
    "    List of precisions for each set of results\n",
    "    \"\"\"\n",
    "    \n",
    "    edge_set = set(G.edges())\n",
    "    AUC = []\n",
    "    for result_set in results:\n",
    "        true_edges = []\n",
    "        false_edges = []\n",
    "        for (x,y,data) in result_set:\n",
    "            if (x,y) in edge_set:\n",
    "                true_edges.append((x,y,data))\n",
    "            else:\n",
    "                false_edges.append((x,y,data))\n",
    "        \n",
    "        random.shuffle(true_edges)\n",
    "        random.shuffle(false_edges)\n",
    "        n = len(true_edges)\n",
    "        n_better = 0.0\n",
    "        n_same = 0.0\n",
    "        for i in range(0, n):\n",
    "            if true_edges[i][2]['score'] > false_edges[i][2]['score']:\n",
    "                n_better += 1.0\n",
    "            if true_edges[i][2]['score'] == false_edges[i][2]['score']:\n",
    "                n_same += 1.0\n",
    "        AUC.append((n_better + 0.5*n_same)/n)\n",
    "    return AUC\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time acm_results = k_fold_validate(GCC, 5, average_commute_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time lhn2_results = k_fold_validate(GCC, 5, leicht_holme_newman_global, alpha=.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time lp_results = k_fold_validate(GCC, 5, local_path_index, epsilon=0.01, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"LP:\"\n",
    "print sum(precision(GCC, lp_results, 50))/5\n",
    "print sum(AUC(GCC, lp_results))/5\n",
    "print \"ACM:\"\n",
    "print sum(precision(GCC, acm_results, 50))/5\n",
    "print sum(AUC(GCC, acm_results))/5\n",
    "print \"LHN2:\"\n",
    "print sum(precision(GCC, lhn2_results, 50))/5\n",
    "print sum(AUC(GCC, lhn2_results))/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time katz_results = k_fold_validate(GCC, 5, katz_index, beta=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Katz:\"\n",
    "print sum(precision(GCC, katz_results, 50))/5\n",
    "print sum(AUC(GCC, katz_results))/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cn_results = k_fold_validate(GCC, 5, common_neighbors)\n",
    "jaccard_results = k_fold_validate(GCC, 5, jaccard)\n",
    "salton_results = k_fold_validate(GCC, 5, salton)\n",
    "pa_results = k_fold_validate(GCC, 5, preferential_attachment)\n",
    "hdi_results = k_fold_validate(GCC, 5, hub_depressed)\n",
    "hpi_results = k_fold_validate(GCC, 5, hub_promoted)\n",
    "ra_results = k_fold_validate(GCC, 5, resource_allocation)\n",
    "aa_results = k_fold_validate(GCC, 5, adamic_adar)\n",
    "sorensen_results = k_fold_validate(GCC, 5, sorensen)\n",
    "lhn_results = k_fold_validate(GCC,5, leicht_holme_newman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CN:\n",
      "0.986666666667\n",
      "0.843005006126\n",
      "Jaccard:\n",
      "0.902666666667\n",
      "0.841042624219\n",
      "Salton:\n",
      "0.914666666667\n",
      "0.842409723317\n",
      "Preferential Attachment:\n",
      "0.964\n",
      "0.7932015242\n",
      "Hub depressed:\n",
      "0.921333333333\n",
      "0.842064110341\n",
      "Hub promoted:\n",
      "0.905333333333\n",
      "0.840561167484\n",
      "Resource allocation:\n",
      "0.965333333333\n",
      "0.84200668027\n",
      "Adamic/Adar:\n",
      "0.976\n",
      "0.843360593439\n",
      "Sorensen:\n",
      "0.913333333333\n",
      "0.840756739314\n",
      "LHN:\n",
      "0.904\n",
      "0.842052928104\n"
     ]
    }
   ],
   "source": [
    "print \"CN:\"\n",
    "print sum(precision(GCC, cn_results, 150))/5\n",
    "print sum(AUC(GCC, cn_results))/5\n",
    "print \"Jaccard:\"\n",
    "print sum(precision(GCC, jaccard_results, 150))/5\n",
    "print sum(AUC(GCC, jaccard_results))/5\n",
    "print \"Salton:\"\n",
    "print sum(precision(GCC, salton_results, 150))/5\n",
    "print sum(AUC(GCC, salton_results))/5\n",
    "print \"Preferential Attachment:\"\n",
    "print sum(precision(GCC, pa_results, 150))/5\n",
    "print sum(AUC(GCC, pa_results))/5\n",
    "print \"Hub depressed:\"\n",
    "print sum(precision(GCC, hdi_results, 150))/5\n",
    "print sum(AUC(GCC, hdi_results))/5\n",
    "print \"Hub promoted:\"\n",
    "print sum(precision(GCC, hpi_results, 150))/5\n",
    "print sum(AUC(GCC, hpi_results))/5\n",
    "print \"Resource allocation:\"\n",
    "print sum(precision(GCC, ra_results, 150))/5\n",
    "print sum(AUC(GCC, ra_results))/5\n",
    "print \"Adamic/Adar:\"\n",
    "print sum(precision(GCC, aa_results, 150))/5\n",
    "print sum(AUC(GCC, aa_results))/5\n",
    "print \"Sorensen:\"\n",
    "print sum(precision(GCC, sorensen_results, 150))/5\n",
    "print sum(AUC(GCC, sorensen_results))/5\n",
    "print \"LHN:\"\n",
    "print sum(precision(GCC, lhn_results, 150))/5\n",
    "print sum(AUC(GCC, lhn_results))/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cn_results = k_fold_validate(GCC, 5, common_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CN:\n",
      "0.987587312493\n",
      "5973.0\n",
      "2704.0\n",
      "6020.0\n",
      "2661.0\n",
      "5996.0\n",
      "2687.0\n",
      "6010.0\n",
      "2658.0\n",
      "5991.0\n",
      "2665.0\n",
      "0.841593882605\n"
     ]
    }
   ],
   "source": [
    "print \"CN:\"\n",
    "print sum(precision(GCC, cn_results, 8733))/5\n",
    "print sum(AUC(GCC, cn_results))/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
